{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\borin\\anaconda3\\envs\\coursework\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#Tweets Machine Learning Coursework \n",
    "#COMP3222\n",
    "#\n",
    "\n",
    "# To support both python 2 and 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# These two lines are required to use Tensorflow 1\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# To plot nice figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Clear tensorflow's and reset seed\n",
    "def reset_graph(seed=None):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#get data\n",
    "def getData(file_name):\n",
    "    dict_data = {}\n",
    "    data_file = open(file_name, \"r\", encoding=\"utf8\")\n",
    "    raw_data_txt = data_file.readlines()\n",
    "    raw_data = []\n",
    "    for data in raw_data_txt:\n",
    "        raw_data.append(data.split(\"\\t\"))\n",
    "    raw_data.pop(0)\n",
    "\n",
    "    target_text = [data[6] for data in raw_data]\n",
    "    dict_data['text'] = np.array([tweet[1] for tweet in raw_data])\n",
    "    dict_data['imageIds'] = np.array([tweet[3] for tweet in raw_data])\n",
    "    dict_data['timestamp'] = np.array([tweet[5] for tweet in raw_data])\n",
    "    dict_data['label'] = np.array([1 if target=='real' or target =='real\\n' else 0 for target in target_text])\n",
    "\n",
    "    return dict_data\n",
    "\n",
    "df_train = pd.DataFrame.from_dict(getData(\"mediaeval-2015-trainingset.txt\"))\n",
    "df_test = pd.DataFrame.from_dict(getData(\"mediaeval-2015-testset.txt\"))\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file pre-processing to csv\n",
    "#df_test.to_csv('test_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train_v2.2.csv\")\n",
    "df_train = df_train.iloc[:, 1:]\n",
    "df_train\n",
    "df_train.text = df_train.text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"test_v2.2.csv\")\n",
    "df_test = df_test.iloc[:, 1:]\n",
    "df_test\n",
    "df_test.text = df_test.text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find all the different languages\n",
    "#https://meta.wikimedia.org/wiki/Template:List_of_language_names_ordered_by_code conversion of symbol to langauge table\n",
    "from langdetect import detect\n",
    "\n",
    "def language_detect(data_frame):\n",
    "    languages = []\n",
    "    for tweet in data_frame['text'].values:\n",
    "        try:\n",
    "            languages.append(detect(tweet))\n",
    "        except:\n",
    "            languages.append('en')\n",
    "\n",
    "    data_frame.insert(3, \"language\", languages, True)\n",
    "\n",
    "    languages_seen = set(languages)\n",
    "    print(\"Languages seen: \", languages_seen)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retweet_detection(data_frame):\n",
    "    retweets = [1 if tweet.count(\"RT\") >= 1 else 0 for tweet in data_frame[\"text\"].values]\n",
    "    data_frame.insert(4, \"retweet\", retweets, True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def translate(data_frame):\n",
    "    translate_text = {}\n",
    "    for i in range(len(data_frame)):\n",
    "        if(data_frame.iloc[i]['language'] != 'en'):\n",
    "            translate_text[data_frame.iloc[i]['text']] = GoogleTranslator(source='auto', target='en').translate(data_frame.iloc[i]['text'])\n",
    "            #print(i, data_frame.iloc[i]['language'], GoogleTranslator(source='auto', target='en').translate(df_train.iloc[i]['text']))\n",
    "\n",
    "    data_frame[\"text\"].replace(translate_text, inplace=True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>imageIds</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>language</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>profane</th>\n",
       "      <th>retweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>remember movie day tomorrow reminds happening ...</td>\n",
       "      <td>VB NN NN NN VBZ VBG NN NN</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>1351550041</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>look sandy ny tremendous image hurricane looks...</td>\n",
       "      <td>NN JJ RB JJ NN NN VBZ IN NN NN CD JJ NN</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>1351537883</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good photo hurricane sandy reminds movie indep...</td>\n",
       "      <td>JJ NN NN JJ VBZ NN NN NN VBZ NN</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>1351534268</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scary shit hurricane ny</td>\n",
       "      <td>JJ VBD NN NN</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>1351538133</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fave place world nyc hurricane sandy statueofl...</td>\n",
       "      <td>JJ NN NN NN NN NN NN</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>1351543562</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14478</th>\n",
       "      <td>slaps tweetdeck pigfish http</td>\n",
       "      <td>NNS VBP JJ NN</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>-1</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14479</th>\n",
       "      <td>new species fish found brazil really good phot...</td>\n",
       "      <td>JJ NNS VBP VBN IN RB JJ NN VBP NN</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>-1</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14480</th>\n",
       "      <td>call pigfish http</td>\n",
       "      <td>VB JJ NN</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>-1</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14481</th>\n",
       "      <td>pigfish shark pork fish http co hqzwghydef</td>\n",
       "      <td>JJ NN NN JJ NN NN NN</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>-1</td>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14482</th>\n",
       "      <td>decide fish meat pigfish http</td>\n",
       "      <td>RB JJ NN JJ NN</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>-1</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14483 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      remember movie day tomorrow reminds happening ...   \n",
       "1      look sandy ny tremendous image hurricane looks...   \n",
       "2      good photo hurricane sandy reminds movie indep...   \n",
       "3                                scary shit hurricane ny   \n",
       "4      fave place world nyc hurricane sandy statueofl...   \n",
       "...                                                  ...   \n",
       "14478                       slaps tweetdeck pigfish http   \n",
       "14479  new species fish found brazil really good phot...   \n",
       "14480                                  call pigfish http   \n",
       "14481         pigfish shark pork fish http co hqzwghydef   \n",
       "14482                      decide fish meat pigfish http   \n",
       "\n",
       "                                           pos        imageIds   timestamp  \\\n",
       "0                    VB NN NN NN VBZ VBG NN NN  sandyA_fake_46  1351550041   \n",
       "1      NN JJ RB JJ NN NN VBZ IN NN NN CD JJ NN  sandyA_fake_09  1351537883   \n",
       "2              JJ NN NN JJ VBZ NN NN NN VBZ NN  sandyA_fake_09  1351534268   \n",
       "3                                 JJ VBD NN NN  sandyA_fake_29  1351538133   \n",
       "4                         JJ NN NN NN NN NN NN  sandyA_fake_15  1351543562   \n",
       "...                                        ...             ...         ...   \n",
       "14478                            NNS VBP JJ NN      pigFish_01          -1   \n",
       "14479        JJ NNS VBP VBN IN RB JJ NN VBP NN      pigFish_01          -1   \n",
       "14480                                 VB JJ NN      pigFish_01          -1   \n",
       "14481                     JJ NN NN JJ NN NN NN      pigFish_01          -1   \n",
       "14482                           RB JJ NN JJ NN      pigFish_01          -1   \n",
       "\n",
       "      language  num_emoji  num_hashtags  profane  retweet  label  \n",
       "0           es          0             1        0        0      0  \n",
       "1           es          0             0        0        1      0  \n",
       "2           es          0             2        0        0      0  \n",
       "3           en          0             2        1        0      0  \n",
       "4           en          1             4        0        0      0  \n",
       "...        ...        ...           ...      ...      ...    ...  \n",
       "14478       en          0             0        0        0      0  \n",
       "14479       en          0             0        0        0      0  \n",
       "14480       en          0             1        0        0      0  \n",
       "14481       it          0             0        0        0      0  \n",
       "14482       en          0             1        0        0      0  \n",
       "\n",
       "[14483 rows x 10 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from profanity_filter import ProfanityFilter\n",
    "\n",
    "def profanity_detection(data_frame):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    profanity_filter = ProfanityFilter(nlps={'en': nlp})  # reuse spacy Language (optional)\n",
    "    nlp.add_pipe(profanity_filter.spacy_component, last=True)\n",
    "\n",
    "    doc = nlp('This is fuck shit!')\n",
    "\n",
    "    doc._.is_profane\n",
    "    profanity_check = []\n",
    "    count = 0\n",
    "    for tweet in data_frame['text'].values:\n",
    "        text = nlp(tweet)\n",
    "        if(text._.is_profane):\n",
    "            profanity_check.append(1)\n",
    "        else: \n",
    "            profanity_check.append(0)\n",
    "        \n",
    "        #print(count, text._.is_profane)\n",
    "        count += 1\n",
    "    \n",
    "    data_frame.insert(4, \"profane\", profanity_check, True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numofhashtags(data_frame):\n",
    "    num_hashtags = []\n",
    "    for i in range(len(data_frame)):\n",
    "        num_hashtags.append(data_frame.iloc[i]['text'].count(\"#\"))\n",
    "    #maybe count number of punctuation as well.\n",
    "    data_frame.insert(4, \"num_hashtags\", num_hashtags, True)\n",
    "    return data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def emoji_detect(data_frame):\n",
    "    num_emojis = []\n",
    "    for i in range(len(data_frame)):\n",
    "        emoji_count = emoji.emoji_count(data_frame.iloc[i]['text'])\n",
    "        num_emojis.append(emoji_count)\n",
    "    \n",
    "    data_frame.insert(4, \"num_emoji\", num_emojis, True)\n",
    "    return data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def parseText(data_frame):\n",
    "    parsed_text = {}\n",
    "    for i in range(len(data_frame)):\n",
    "        parsed = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",data_frame.iloc[i]['text']).split())\n",
    "        parsed_text[data_frame.iloc[i]['text']] = parsed.lower()\n",
    "        #print(i, parsed)\n",
    "    data_frame[\"text\"].replace(parsed_text, inplace=True)\n",
    "    return data_frame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based of https://craft.co/cnn/competitors\n",
    "#\n",
    "def new_companies(data_frame):\n",
    "    news_companies =[\"cnn\", \"bbc\", \"nbc\", \"new york times\", \"wall street journal\", \" ap \", \"fox\", \"cnbc\", \"daily mail\"]\n",
    "    news_found = []\n",
    "    for tweet in data_frame['text'].values:\n",
    "        tweet = tweet.lower()\n",
    "        if(any(news in tweet for news in news_companies) or (tweet.find('ap') == 0) or (tweet.find('ap') == len(tweet)-2) ) :\n",
    "            news_found.append(1)\n",
    "        else:\n",
    "            news_found.append(0)\n",
    "    data_frame.insert(4, \"news_company_found\", news_found, True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#location detector too much noise picked up\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "def location_detector():\n",
    "    geolocator = Nominatim(user_agent=\"COMP322-MLT\")\n",
    "    x = \"My fave place in the world nyc hurricane sandy times sqaure\"\n",
    "    x = x.split(' ')\n",
    "    for i in range(len(x)):\n",
    "        try:\n",
    "            location = geolocator.geocode(x[i])\n",
    "            print(location.address)\n",
    "        except:\n",
    "            print(\"Notfound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run hashtags if first time running on machine\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def stopWordRemoval(data_frame):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = {}\n",
    "    for i in range(len(data_frame)):\n",
    "        word_tokens = word_tokenize(data_frame.iloc[i]['text'])\n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "        new_sentence = \" \".join(filtered_sentence)\n",
    "        filtered_text[data_frame.iloc[i]['text']] = new_sentence\n",
    "        #print(i, new_sentence)\n",
    "    data_frame[\"text\"].replace(filtered_text, inplace=True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "def misspelling_detection(data_frame):\n",
    "    spell = SpellChecker()\n",
    "    misspelled_count = []\n",
    "    for i in range(len(data_frame)):\n",
    "        word_list = word_tokenize(data_frame.iloc[i]['text'])\n",
    "        misspelled = spell.unknown(word_list)\n",
    "        misspelled_count.append(len(misspelled))\n",
    "        #print(i, len(misspelled), misspelled))\n",
    "    data_frame.insert(4, \"misspellings\", misspelled_count, True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "fake_words = defaultdict(lambda: 0)\n",
    "real_words = defaultdict(lambda: 0)\n",
    "total_words = defaultdict(lambda: 0)\n",
    "\n",
    "for i in range(len(df_train)):\n",
    "    text = str(df_train.iloc[i]['text']).lower()\n",
    "    text = text.split()\n",
    "    if(df_train.iloc[i]['label'] == 0):\n",
    "        for j in range(len(text)):\n",
    "            fake_words[text[j]] += 1\n",
    "            total_words[text[j]] += 1\n",
    "    else:\n",
    "        for j in range(len(text)):\n",
    "            real_words[text[j]] += 1\n",
    "            total_words[text[j]] += 1\n",
    "\n",
    "\n",
    "real_words = dict(sorted(real_words.items(), reverse=True, key=lambda item: item[1]))\n",
    "fake_words = dict(sorted(fake_words.items(), reverse=True, key=lambda item: item[1]))\n",
    "total_words = dict(sorted(total_words.items(), reverse=False, key=lambda item: item[1]))\n",
    "\n",
    "words = list(total_words.keys())\n",
    "for w in words:\n",
    "    if(total_words[w] < 100):\n",
    "        del total_words[w]\n",
    "        try:\n",
    "            del real_words[w]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            del fake_words[w]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "rw = Counter(real_words)\n",
    "fw = Counter(fake_words)\n",
    "fake_res = fw - rw\n",
    "real_res = rw - fw\n",
    "\n",
    "real_res = {k: real_res[k]/total_words[k] for k in total_words.keys() & real_res}\n",
    "fake_res = {k: fake_res[k]/total_words[k] for k in total_words.keys() & fake_res}\n",
    "\n",
    "real_res = dict(sorted(real_res.items(), reverse=True, key=lambda item: item[1]))\n",
    "fake_res = dict(sorted(fake_res.items(), reverse=True, key=lambda item: item[1]))\n",
    "\n",
    "\n",
    "sig_real_words = list(k for k, v in real_res.items() if v > 0.5)\n",
    "sig_fake_words = list(k for k, v in fake_res.items() if v > 0.5)\n",
    "\n",
    "def significance(data_frame, sig_words, clmn_name):\n",
    "    sig_found = []\n",
    "    for tweet in data_frame['text'].values:\n",
    "        try:\n",
    "            tweet = tweet.lower()\n",
    "            if(any(word in tweet for word in sig_words)):\n",
    "                sig_found.append(1)\n",
    "            else:\n",
    "                sig_found.append(0)\n",
    "        except:\n",
    "            sig_found.append(0)\n",
    "    data_frame.insert(4, clmn_name, sig_found, True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def n_grams(data_frame, n):\n",
    "    a = []\n",
    "    for i in range(len(data_frame)):\n",
    "        sentence = data_frame.iloc[i]['text']\n",
    "\n",
    "        ng = ngrams(sentence.split(), n)\n",
    "        x = []\n",
    "        for grams in ng:\n",
    "            x.append(grams)\n",
    "        a.append(x)\n",
    "    data_frame.insert(3, ng+\"-gram\", a, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages seen:  {'ta', 'sq', 'lt', 'el', 'ar', 'so', 'tl', 'pl', 'hi', 'de', 'ro', 'ru', 'th', 'fr', 'ko', 'es', 'ca', 'en', 'ja', 'pt', 'af', 'id', 'cy', 'nl', 'et', 'it', 'fi', 'sv', 'te', 'vi', 'tr', 'bg', 'hr'}\n",
      "Language Detection Done\n",
      "Translation Done\n",
      "Retweet Detection Done\n",
      "News companies Detection Done\n",
      "Hashtag count Done\n",
      "Emoji count Done\n",
      "Parsing Done\n",
      "Stopwprd removal Done\n",
      "Misspelling Count Done\n",
      "Profanity Detection Done\n",
      "Preprocessing done\n"
     ]
    }
   ],
   "source": [
    "df_test = language_detect(df_test)\n",
    "print(\"Language Detection Done\")\n",
    "df_test = translate(df_test)\n",
    "print(\"Translation Done\")\n",
    "df_test = retweet_detection(df_test)\n",
    "print(\"Retweet Detection Done\")\n",
    "df_test = new_companies(df_test)\n",
    "print(\"News companies Detection Done\")\n",
    "df_test = numofhashtags(df_test)\n",
    "print(\"Hashtag count Done\")\n",
    "df_test = emoji_detect(df_test)\n",
    "print(\"Emoji count Done\")\n",
    "df_test = parseText(df_test)\n",
    "print(\"Parsing Done\")\n",
    "df_test = stopWordRemoval(df_test)\n",
    "print(\"Stopwprd removal Done\")\n",
    "df_test = misspelling_detection(df_test)\n",
    "print(\"Misspelling Count Done\")\n",
    "df_test = profanity_detection(df_test)\n",
    "print(\"Profanity Detection Done\")\n",
    "df_test = significance(df_test, sig_real_words, \"significant_real\")\n",
    "print(\"Real Significant Words Detected\")\n",
    "df_test = significance(df_test, sig_fake_words, \"significant_fake\")\n",
    "print(\"Fake Significant Words Detected\")\n",
    "\n",
    "print(\"Preprocessing done\")\n",
    "#before passing the text as unigram .lower everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>imageIds</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>language</th>\n",
       "      <th>profane</th>\n",
       "      <th>misspellings</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kereeen rt eclipse iss</td>\n",
       "      <td>JJ NN NN NN</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>1426844743</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>absolutely beautiful rt eclipse iss</td>\n",
       "      <td>RB JJ NN NN NN</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>1426849442</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eclipse iss 3 20 wow amazing</td>\n",
       "      <td>NN VBZ CD CD NN NN</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>1426853406</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eclipse iss</td>\n",
       "      <td>NN NN</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>1426842761</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eclipse seen iss something else divine creatio...</td>\n",
       "      <td>NN VBN JJ NN RB JJ NN NN</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>1426873451</td>\n",
       "      <td>fr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>zdf presenter confesses rigged video varoufaki...</td>\n",
       "      <td>NN NN NNS VBD NN NN NN JJ NN IN</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>1426744184</td>\n",
       "      <td>fr</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>oh kleine liars zdf presenter confesses faked ...</td>\n",
       "      <td>UH NN NNS VBP NN NNS VBD JJ NN NN</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>1426744302</td>\n",
       "      <td>fr</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>zdf program confirm varoufakis video montage</td>\n",
       "      <td>NN NN NN VBZ JJ NN</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>1426757001</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>11 34 almost noon big confusion varoufakis vid...</td>\n",
       "      <td>CD CD RB RB JJ NN NN NN NN JJ NN NN NN</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>1426761320</td>\n",
       "      <td>de</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>sorry english subtitles full english greek sub...</td>\n",
       "      <td>JJ JJ NNS JJ JJ NN NNS JJ RB VBP</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>1426713665</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3781 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0                                kereeen rt eclipse iss   \n",
       "1                   absolutely beautiful rt eclipse iss   \n",
       "2                          eclipse iss 3 20 wow amazing   \n",
       "3                                           eclipse iss   \n",
       "4     eclipse seen iss something else divine creatio...   \n",
       "...                                                 ...   \n",
       "3776  zdf presenter confesses rigged video varoufaki...   \n",
       "3777  oh kleine liars zdf presenter confesses faked ...   \n",
       "3778       zdf program confirm varoufakis video montage   \n",
       "3779  11 34 almost noon big confusion varoufakis vid...   \n",
       "3780  sorry english subtitles full english greek sub...   \n",
       "\n",
       "                                         pos      imageIds   timestamp  \\\n",
       "0                                JJ NN NN NN   eclipse_01   1426844743   \n",
       "1                             RB JJ NN NN NN   eclipse_01   1426849442   \n",
       "2                         NN VBZ CD CD NN NN   eclipse_01   1426853406   \n",
       "3                                      NN NN   eclipse_01   1426842761   \n",
       "4                   NN VBN JJ NN RB JJ NN NN    eclipse_01  1426873451   \n",
       "...                                      ...           ...         ...   \n",
       "3776         NN NN NNS VBD NN NN NN JJ NN IN  varoufakis_1  1426744184   \n",
       "3777       UH NN NNS VBP NN NNS VBD JJ NN NN  varoufakis_1  1426744302   \n",
       "3778                      NN NN NN VBZ JJ NN  varoufakis_1  1426757001   \n",
       "3779  CD CD RB RB JJ NN NN NN NN JJ NN NN NN  varoufakis_1  1426761320   \n",
       "3780        JJ JJ NNS JJ JJ NN NNS JJ RB VBP  varoufakis_1  1426713665   \n",
       "\n",
       "     language  profane  misspellings  num_emoji  label  \n",
       "0          en        0             2          0      0  \n",
       "1          en        0             1          0      0  \n",
       "2          en        0             0          0      0  \n",
       "3          en        0             0          0      0  \n",
       "4          fr        0             0          1      0  \n",
       "...       ...      ...           ...        ...    ...  \n",
       "3776       fr        0             2          0      0  \n",
       "3777       fr        0             2          0      0  \n",
       "3778       es        0             2          0      0  \n",
       "3779       de        0             4          2      0  \n",
       "3780       en        0             1          0      0  \n",
       "\n",
       "[3781 rows x 9 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>imageIds</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>language</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>profane</th>\n",
       "      <th>retweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>VB NN NN NN VBZ VBG NN NN</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>1351550041</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>NN JJ RB JJ NN NN VBZ IN NN NN CD JJ NN</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>1351537883</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>JJ NN NN JJ VBZ NN NN NN VBZ NN</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>1351534268</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>JJ VBD NN NN</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>1351538133</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>JJ NN NN NN NN NN NN</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>1351543562</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14478</th>\n",
       "      <td>[]</td>\n",
       "      <td>NNS VBP JJ NN</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>-1</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14479</th>\n",
       "      <td>[]</td>\n",
       "      <td>JJ NNS VBP VBN IN RB JJ NN VBP NN</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>-1</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14480</th>\n",
       "      <td>[]</td>\n",
       "      <td>VB JJ NN</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>-1</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14481</th>\n",
       "      <td>[]</td>\n",
       "      <td>JJ NN NN JJ NN NN NN</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>-1</td>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14482</th>\n",
       "      <td>[]</td>\n",
       "      <td>RB JJ NN JJ NN</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>-1</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14483 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      text                                      pos        imageIds  \\\n",
       "0       []                VB NN NN NN VBZ VBG NN NN  sandyA_fake_46   \n",
       "1       []  NN JJ RB JJ NN NN VBZ IN NN NN CD JJ NN  sandyA_fake_09   \n",
       "2       []          JJ NN NN JJ VBZ NN NN NN VBZ NN  sandyA_fake_09   \n",
       "3       []                             JJ VBD NN NN  sandyA_fake_29   \n",
       "4       []                     JJ NN NN NN NN NN NN  sandyA_fake_15   \n",
       "...    ...                                      ...             ...   \n",
       "14478   []                            NNS VBP JJ NN      pigFish_01   \n",
       "14479   []        JJ NNS VBP VBN IN RB JJ NN VBP NN      pigFish_01   \n",
       "14480   []                                 VB JJ NN      pigFish_01   \n",
       "14481   []                     JJ NN NN JJ NN NN NN      pigFish_01   \n",
       "14482   []                           RB JJ NN JJ NN      pigFish_01   \n",
       "\n",
       "        timestamp language  num_emoji  num_hashtags  profane  retweet  label  \n",
       "0      1351550041       es          0             1        0        0      0  \n",
       "1      1351537883       es          0             0        0        1      0  \n",
       "2      1351534268       es          0             2        0        0      0  \n",
       "3      1351538133       en          0             2        1        0      0  \n",
       "4      1351543562       en          1             4        0        0      0  \n",
       "...           ...      ...        ...           ...      ...      ...    ...  \n",
       "14478          -1       en          0             0        0        0      0  \n",
       "14479          -1       en          0             0        0        0      0  \n",
       "14480          -1       en          0             1        0        0      0  \n",
       "14481          -1       it          0             0        0        0      0  \n",
       "14482          -1       en          0             1        0        0      0  \n",
       "\n",
       "[14483 rows x 10 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>imageIds</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>language</th>\n",
       "      <th>significant_fake</th>\n",
       "      <th>significant_real</th>\n",
       "      <th>profane</th>\n",
       "      <th>misspellings</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>news_company_found</th>\n",
       "      <th>retweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>remember movie Day Tomorrow reminds happening ...</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Look Sandy NY Tremendous image hurricane Looks...</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good photo Hurricane Sandy reminds movie Indep...</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit hurricane NY</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fave place world nyc hurricane sandy statueofl...</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14478</th>\n",
       "      <td>slaps TweetDeck PigFish http co pyHcJn0jwA</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Tue Mar 11 03: 48: 36 +0000 2014</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14479</th>\n",
       "      <td>New Species Fish found Brazil Really good Phot...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Mon Mar 10 18: 09: 26 +0000 2014</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14480</th>\n",
       "      <td>call pigFISH http co 4Bml62OD15</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Mon Mar 10 10: 59: 45 +0000 2014</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14481</th>\n",
       "      <td>Pigfish shark pork fish http co hQzWGhyDef</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Sun Mar 09 20: 07: 10 +0000 2014</td>\n",
       "      <td>it</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14482</th>\n",
       "      <td>decide fish meat Pigfish http co 5JBtF54cmg</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Sun Mar 09 16: 36: 09 +0000 2014</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14483 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text        imageIds  \\\n",
       "0      remember movie Day Tomorrow reminds happening ...  sandyA_fake_46   \n",
       "1      Look Sandy NY Tremendous image hurricane Looks...  sandyA_fake_09   \n",
       "2      Good photo Hurricane Sandy reminds movie Indep...  sandyA_fake_09   \n",
       "3                                Scary shit hurricane NY  sandyA_fake_29   \n",
       "4      fave place world nyc hurricane sandy statueofl...  sandyA_fake_15   \n",
       "...                                                  ...             ...   \n",
       "14478         slaps TweetDeck PigFish http co pyHcJn0jwA      pigFish_01   \n",
       "14479  New Species Fish found Brazil Really good Phot...      pigFish_01   \n",
       "14480                    call pigFISH http co 4Bml62OD15      pigFish_01   \n",
       "14481         Pigfish shark pork fish http co hQzWGhyDef      pigFish_01   \n",
       "14482        decide fish meat Pigfish http co 5JBtF54cmg      pigFish_01   \n",
       "\n",
       "                              timestamp language  significant_fake  \\\n",
       "0        Mon Oct 29 22:34:01 +0000 2012       es                 1   \n",
       "1        Mon Oct 29 19:11:23 +0000 2012       es                 1   \n",
       "2        Mon Oct 29 18:11:08 +0000 2012       es                 1   \n",
       "3        Mon Oct 29 19:15:33 +0000 2012       en                 1   \n",
       "4        Mon Oct 29 20:46:02 +0000 2012       en                 1   \n",
       "...                                 ...      ...               ...   \n",
       "14478  Tue Mar 11 03: 48: 36 +0000 2014       en                 0   \n",
       "14479  Mon Mar 10 18: 09: 26 +0000 2014       en                 1   \n",
       "14480  Mon Mar 10 10: 59: 45 +0000 2014       en                 0   \n",
       "14481  Sun Mar 09 20: 07: 10 +0000 2014       it                 1   \n",
       "14482  Sun Mar 09 16: 36: 09 +0000 2014       en                 0   \n",
       "\n",
       "       significant_real  profane  misspellings  num_emoji  num_hashtags  \\\n",
       "0                     0        0             0          0             1   \n",
       "1                     0        0             1          0             0   \n",
       "2                     0        0             1          0             2   \n",
       "3                     0        1             0          0             2   \n",
       "4                     0        0             2          1             4   \n",
       "...                 ...      ...           ...        ...           ...   \n",
       "14478                 0        0             4          0             0   \n",
       "14479                 0        0             2          0             0   \n",
       "14480                 0        0             3          0             1   \n",
       "14481                 0        0             3          0             0   \n",
       "14482                 0        0             3          0             1   \n",
       "\n",
       "       news_company_found  retweet  label  \n",
       "0                       0        0      0  \n",
       "1                       0        1      0  \n",
       "2                       0        0      0  \n",
       "3                       0        0      0  \n",
       "4                       0        0      0  \n",
       "...                   ...      ...    ...  \n",
       "14478                   0        0      0  \n",
       "14479                   0        0      0  \n",
       "14480                   0        0      0  \n",
       "14481                   0        0      0  \n",
       "14482                   0        0      0  \n",
       "\n",
       "[14483 rows x 13 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train[['text', 'imageIds', 'timestamp', 'language', 'significant_fake', 'significant_real', 'profane', 'misspellings', 'num_emoji', 'num_hashtags', 'news_company_found', 'retweet', 'label']]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_features(data_frame):\n",
    "    features = []\n",
    "    for i in range(len(data_frame)):\n",
    "        temp = []\n",
    "        #temp.append(data_frame.iloc[i]['significant_fake'])\n",
    "        #temp.append(data_frame.iloc[i]['significant_real'])\n",
    "        temp.append(data_frame.iloc[i]['profane'])\n",
    "        #temp.append(data_frame.iloc[i]['misspellings'])\n",
    "        #temp.append(data_frame.iloc[i]['num_emoji'])\n",
    "        #temp.append(data_frame.iloc[i]['num_hashtags'])\n",
    "        #temp.append(data_frame.iloc[i]['news_company_found'])\n",
    "        features.append(temp)\n",
    "    features = np.array(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_train['text']\n",
    "texts.append(df_test['text'])\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "df_train['text'] = tokenizer.texts_to_sequences(df_train['text'])\n",
    "MAX_TWEET_LENGTH = 16\n",
    "\n",
    "df_test['text'] = tokenizer.texts_to_sequences(df_test['text'])\n",
    "\n",
    "biggest_token = max(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = df_train.pop('text')\n",
    "X_test_text = df_test.pop('text')\n",
    "\n",
    "X_train_text = tf.keras.preprocessing.sequence.pad_sequences(X_train_text, maxlen=MAX_TWEET_LENGTH, padding='post', truncating='post')\n",
    "X_test_text = tf.keras.preprocessing.sequence.pad_sequences(X_test_text, maxlen=MAX_TWEET_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = pull_features(df_train)\n",
    "X_test_features = pull_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3616/126120714.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_features' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_features.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\borin\\anaconda3\\envs\\coursework\\lib\\site-packages\\tensorflow\\python\\keras\\initializers\\initializers_v1.py:58: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "tweet_input (InputLayer)     [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "article_body_embedding (Embe (None, 16, 50)            454500    \n",
      "_________________________________________________________________\n",
      "article_body_conv (Conv1D)   (None, 9, 128)            51328     \n",
      "_________________________________________________________________\n",
      "article_body_pooling (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 75)                9675      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                3800      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 520,344\n",
      "Trainable params: 520,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# feat_inputs = tf.keras.layers.Input(shape=(1,), name='features')\n",
    "# feat_embed = tf.keras.layers.Embedding(10+1, 50, input_length=1, name='feature_embedding')(feat_inputs)\n",
    "# feat_conv = tf.keras.layers.Conv1D(256, 1, name='feature_conv')(feat_embed)\n",
    "# feat_pool = tf.keras.layers.GlobalMaxPool1D(name='feature_pooling')(feat_conv)\n",
    "\n",
    "\n",
    "text_input = tf.keras.layers.Input(shape=(MAX_TWEET_LENGTH,), name='tweet_input')\n",
    "text_embed = tf.keras.layers.Embedding(biggest_token + 1, 50, input_length=MAX_TWEET_LENGTH, name='article_body_embedding')(text_input)\n",
    "text_conv = tf.keras.layers.Conv1D(128, 8, name='article_body_conv')(text_embed)\n",
    "text_pool = tf.keras.layers.GlobalAveragePooling1D(name='article_body_pooling')(text_conv)\n",
    "\n",
    "# concat = tf.keras.layers.concatenate([feat_pool, text_pool])\n",
    "dense_10 = tf.keras.layers.Dense(75, activation='selu')(text_pool)\n",
    "dense_5= tf.keras.layers.Dense(50, activation='selu')(dense_10)\n",
    "dense_2= tf.keras.layers.Dense(20, activation='selu')(dense_5)\n",
    "out_layer = tf.keras.layers.Dense(1, activation='sigmoid')(dense_2)\n",
    "model = tf.keras.models.Model(inputs=[text_input], outputs=out_layer)\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14483 samples, validate on 3781 samples\n",
      "Epoch 1/100\n",
      "14483/14483 [==============================] - ETA: 0s - loss: 0.3578 - acc: 0.8385WARNING:tensorflow:From C:\\Users\\borin\\anaconda3\\envs\\coursework\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0080s). Check your callbacks.\n",
      "14483/14483 [==============================] - 3s 202us/sample - loss: 0.3578 - acc: 0.8385 - val_loss: 0.9139 - val_acc: 0.4806\n",
      "Epoch 2/100\n",
      "14483/14483 [==============================] - 3s 174us/sample - loss: 0.1698 - acc: 0.9338 - val_loss: 1.0724 - val_acc: 0.5073\n",
      "Epoch 3/100\n",
      "14483/14483 [==============================] - 3s 176us/sample - loss: 0.1183 - acc: 0.9559 - val_loss: 0.8845 - val_acc: 0.5988\n",
      "Epoch 4/100\n",
      "14483/14483 [==============================] - 3s 180us/sample - loss: 0.0971 - acc: 0.9637 - val_loss: 1.0101 - val_acc: 0.6400\n",
      "Epoch 5/100\n",
      "14483/14483 [==============================] - 3s 182us/sample - loss: 0.0845 - acc: 0.9680 - val_loss: 0.7376 - val_acc: 0.8098\n",
      "Epoch 6/100\n",
      "14483/14483 [==============================] - 3s 187us/sample - loss: 0.0793 - acc: 0.9698 - val_loss: 0.8047 - val_acc: 0.8059\n",
      "Epoch 7/100\n",
      "14483/14483 [==============================] - 3s 186us/sample - loss: 0.0685 - acc: 0.9745 - val_loss: 0.7159 - val_acc: 0.8244\n",
      "Epoch 8/100\n",
      "14483/14483 [==============================] - 2s 166us/sample - loss: 0.0648 - acc: 0.9747 - val_loss: 0.7349 - val_acc: 0.8408\n",
      "Epoch 9/100\n",
      "14483/14483 [==============================] - 2s 167us/sample - loss: 0.0617 - acc: 0.9773 - val_loss: 0.9048 - val_acc: 0.7953\n",
      "Epoch 10/100\n",
      "14483/14483 [==============================] - 2s 170us/sample - loss: 0.0597 - acc: 0.9769 - val_loss: 0.8381 - val_acc: 0.810197\n",
      "Epoch 11/100\n",
      "14483/14483 [==============================] - ETA: 0s - loss: 0.0550 - acc: 0.979 - 2s 172us/sample - loss: 0.0549 - acc: 0.9792 - val_loss: 0.8805 - val_acc: 0.8125\n",
      "Epoch 12/100\n",
      "14483/14483 [==============================] - 2s 173us/sample - loss: 0.0538 - acc: 0.9792 - val_loss: 0.8627 - val_acc: 0.8212\n",
      "Epoch 13/100\n",
      "14483/14483 [==============================] - 2s 170us/sample - loss: 0.0513 - acc: 0.9803 - val_loss: 0.9519 - val_acc: 0.8183\n"
     ]
    }
   ],
   "source": [
    "y_train = df_train['label'].values\n",
    "y_test = df_test['label'].values\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, mode='max', restore_best_weights=True)\n",
    "\n",
    "history = model.fit([X_train_text], y_train, epochs=100, batch_size=128, validation_data=([X_test_text], y_test), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7348672887401523, 0.8407829]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.840782861676805"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.metrics as skl_m\n",
    "\n",
    "preds = model.predict([X_test_text])\n",
    "print(model.evaluate([X_test_text], y_test))\n",
    "\n",
    "y_pred = []\n",
    "for i in range(len(preds)):\n",
    "    if(preds[i]<0.5):\n",
    "        y_pred.append(0)\n",
    "    else:\n",
    "        y_pred.append(1)\n",
    "    \n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "f1 = skl_m.f1_score(y_test, y_pred, average='micro')\n",
    "f1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6ac1c65944d4ad9c277d53219687cbb170d2c6377a780c0883c4434ac8bf052"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('coursework': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
