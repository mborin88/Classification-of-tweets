{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\borin\\anaconda3\\envs\\coursework\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#Tweets Machine Learning Coursework \n",
    "#COMP3222\n",
    "#\n",
    "\n",
    "# To support both python 2 and 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# These two lines are required to use Tensorflow 1\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# To plot nice figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Clear tensorflow's and reset seed\n",
    "def reset_graph(seed=None):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#get data\n",
    "def getData(file_name):\n",
    "    dict_data = {}\n",
    "    data_file = open(file_name, \"r\", encoding=\"utf8\")\n",
    "    raw_data_txt = data_file.readlines()\n",
    "    raw_data = []\n",
    "    for data in raw_data_txt:\n",
    "        raw_data.append(data.split(\"\\t\"))\n",
    "    raw_data.pop(0)\n",
    "\n",
    "    target_text = [data[6] for data in raw_data]\n",
    "    dict_data['text'] = np.array([tweet[1] for tweet in raw_data])\n",
    "    dict_data['imageIds'] = np.array([tweet[3] for tweet in raw_data])\n",
    "    dict_data['timestamp'] = np.array([tweet[5] for tweet in raw_data])\n",
    "    dict_data['label'] = np.array([1 if target=='real' or target =='real\\n' else 0 for target in target_text])\n",
    "\n",
    "    return dict_data\n",
    "\n",
    "df_train = pd.DataFrame.from_dict(getData(\"mediaeval-2015-trainingset.txt\"))\n",
    "df_test = pd.DataFrame.from_dict(getData(\"mediaeval-2015-testset.txt\"))\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file pre-processing to csv\n",
    "# df_test.to_csv('test_v2.2.1.csv')\n",
    "# df_train.to_csv('train_v2.2.1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train_v2.2.0.csv\")\n",
    "df_train = df_train.iloc[:, 1:]\n",
    "df_train\n",
    "df_train.text = df_train.text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"test_v2.2.0.csv\")\n",
    "df_test = df_test.iloc[:, 1:]\n",
    "df_test\n",
    "df_test.text = df_test.text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find all the different languages\n",
    "#https://meta.wikimedia.org/wiki/Template:List_of_language_names_ordered_by_code conversion of symbol to langauge table\n",
    "from langdetect import detect\n",
    "\n",
    "def language_detect(data_frame):\n",
    "    languages = []\n",
    "    for tweet in data_frame['text'].values:\n",
    "        try:\n",
    "            languages.append(detect(tweet))\n",
    "        except:\n",
    "            languages.append('en')\n",
    "\n",
    "    data_frame.insert(3, \"language\", languages, True)\n",
    "\n",
    "    languages_seen = set(languages)\n",
    "    print(\"Languages seen: \", languages_seen)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retweet_detection(data_frame):\n",
    "    retweets = [1 if tweet.count(\"RT\") >= 1 else 0 for tweet in data_frame[\"text\"].values]\n",
    "    data_frame.insert(4, \"retweet\", retweets, True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def translate(data_frame):\n",
    "    translate_text = {}\n",
    "    for i in range(len(data_frame)):\n",
    "        if(data_frame.iloc[i]['language'] != 'en'):\n",
    "            translate_text[data_frame.iloc[i]['text']] = GoogleTranslator(source='auto', target='en').translate(data_frame.iloc[i]['text'])\n",
    "            #print(i, data_frame.iloc[i]['language'], GoogleTranslator(source='auto', target='en').translate(df_train.iloc[i]['text']))\n",
    "\n",
    "    data_frame[\"text\"].replace(translate_text, inplace=True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>imageIds</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>language</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>profane</th>\n",
       "      <th>retweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do you remember the movie: \"The Day After Tomo...</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milenagimon: Look at Sandy in NY! Tremendous ...</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good the photo of Hurricane Sandy, it reminds ...</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My fave place in the world #nyc #hurricane #sa...</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14478</th>\n",
       "      <td>@BobombDom *slaps TweetDeck with the PigFish h...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Tue Mar 11 03: 48: 36 +0000 2014</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14479</th>\n",
       "      <td>New Species of Fish found in Brazil or just Re...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Mon Mar 10 18: 09: 26 +0000 2014</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14480</th>\n",
       "      <td>What do we call this? #pigFISH http: \\/\\/t.co\\...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Mon Mar 10 10: 59: 45 +0000 2014</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14481</th>\n",
       "      <td>Pigfish? And after the shark there is the pork...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Sun Mar 09 20: 07: 10 +0000 2014</td>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14482</th>\n",
       "      <td>For those who can't decide between fish or mea...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Sun Mar 09 16: 36: 09 +0000 2014</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14483 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text        imageIds  \\\n",
       "0      Do you remember the movie: \"The Day After Tomo...  sandyA_fake_46   \n",
       "1      @milenagimon: Look at Sandy in NY! Tremendous ...  sandyA_fake_09   \n",
       "2      Good the photo of Hurricane Sandy, it reminds ...  sandyA_fake_09   \n",
       "3         Scary shit #hurricane #NY http://t.co/e4JLBUfH  sandyA_fake_29   \n",
       "4      My fave place in the world #nyc #hurricane #sa...  sandyA_fake_15   \n",
       "...                                                  ...             ...   \n",
       "14478  @BobombDom *slaps TweetDeck with the PigFish h...      pigFish_01   \n",
       "14479  New Species of Fish found in Brazil or just Re...      pigFish_01   \n",
       "14480  What do we call this? #pigFISH http: \\/\\/t.co\\...      pigFish_01   \n",
       "14481  Pigfish? And after the shark there is the pork...      pigFish_01   \n",
       "14482  For those who can't decide between fish or mea...      pigFish_01   \n",
       "\n",
       "                              timestamp language  num_emoji  num_hashtags  \\\n",
       "0        Mon Oct 29 22:34:01 +0000 2012       es          0             1   \n",
       "1        Mon Oct 29 19:11:23 +0000 2012       es          0             0   \n",
       "2        Mon Oct 29 18:11:08 +0000 2012       es          0             2   \n",
       "3        Mon Oct 29 19:15:33 +0000 2012       en          0             2   \n",
       "4        Mon Oct 29 20:46:02 +0000 2012       en          1             4   \n",
       "...                                 ...      ...        ...           ...   \n",
       "14478  Tue Mar 11 03: 48: 36 +0000 2014       en          0             0   \n",
       "14479  Mon Mar 10 18: 09: 26 +0000 2014       en          0             0   \n",
       "14480  Mon Mar 10 10: 59: 45 +0000 2014       en          0             1   \n",
       "14481  Sun Mar 09 20: 07: 10 +0000 2014       it          0             0   \n",
       "14482  Sun Mar 09 16: 36: 09 +0000 2014       en          0             1   \n",
       "\n",
       "       profane  retweet  label  \n",
       "0            0        0      0  \n",
       "1            0        1      0  \n",
       "2            0        0      0  \n",
       "3            1        0      0  \n",
       "4            0        0      0  \n",
       "...        ...      ...    ...  \n",
       "14478        0        0      0  \n",
       "14479        0        0      0  \n",
       "14480        0        0      0  \n",
       "14481        0        0      0  \n",
       "14482        0        0      0  \n",
       "\n",
       "[14483 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from profanity_filter import ProfanityFilter\n",
    "\n",
    "def profanity_detection(data_frame):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    profanity_filter = ProfanityFilter(nlps={'en': nlp})  # reuse spacy Language (optional)\n",
    "    nlp.add_pipe(profanity_filter.spacy_component, last=True)\n",
    "\n",
    "    doc = nlp('This is fuck shit!')\n",
    "\n",
    "    doc._.is_profane\n",
    "    profanity_check = []\n",
    "    count = 0\n",
    "    for tweet in data_frame['text'].values:\n",
    "        text = nlp(tweet)\n",
    "        if(text._.is_profane):\n",
    "            profanity_check.append(1)\n",
    "        else: \n",
    "            profanity_check.append(0)\n",
    "        \n",
    "        #print(count, text._.is_profane)\n",
    "        count += 1\n",
    "    \n",
    "    data_frame.insert(4, \"profane\", profanity_check, True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numofhashtags(data_frame):\n",
    "    num_hashtags = []\n",
    "    for i in range(len(data_frame)):\n",
    "        num_hashtags.append(data_frame.iloc[i]['text'].count(\"#\"))\n",
    "    #maybe count number of punctuation as well.\n",
    "    data_frame.insert(4, \"num_hashtags\", num_hashtags, True)\n",
    "    return data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def emoji_detect(data_frame):\n",
    "    num_emojis = []\n",
    "    for i in range(len(data_frame)):\n",
    "        emoji_count = emoji.emoji_count(data_frame.iloc[i]['text'])\n",
    "        num_emojis.append(emoji_count)\n",
    "    \n",
    "    data_frame.insert(4, \"num_emoji\", num_emojis, True)\n",
    "    return data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def parseText(data_frame):\n",
    "    parsed_text = {}\n",
    "    for i in range(len(data_frame)):\n",
    "        parsed = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|((\\\\\\/\\\\\\/\\w+.co\\S+)|(\\w+:\\/\\/\\S+))|([^0-9A-Za-z \\t])\",\" \",data_frame.iloc[i]['text']).split())\n",
    "        parsed_text[data_frame.iloc[i]['text']] = parsed.lower()\n",
    "        #print(i, parsed)\n",
    "    data_frame[\"text\"].replace(parsed_text, inplace=True)\n",
    "    return data_frame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based of https://craft.co/cnn/competitors\n",
    "#\n",
    "def news_companies(data_frame):\n",
    "    news_companies =[\"cnn\", \"bbc\", \"nbc\", \"new york times\", \"wall street journal\", \" ap \", \"fox\", \"cnbc\", \"daily mail\"]\n",
    "    news_found = []\n",
    "    for tweet in data_frame['text'].values:\n",
    "        tweet = tweet.lower()\n",
    "        if(any(news in tweet for news in news_companies) or (tweet.find('ap') == 0) or (tweet.find('ap') == len(tweet)-2) ) :\n",
    "            news_found.append(1)\n",
    "        else:\n",
    "            news_found.append(0)\n",
    "    data_frame.insert(4, \"news_company_found\", news_found, True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#location detector too much noise picked up\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "def location_detector():\n",
    "    geolocator = Nominatim(user_agent=\"COMP322-MLT\")\n",
    "    x = \"My fave place in the world nyc hurricane sandy times sqaure\"\n",
    "    x = x.split(' ')\n",
    "    for i in range(len(x)):\n",
    "        try:\n",
    "            location = geolocator.geocode(x[i])\n",
    "            print(location.address)\n",
    "        except:\n",
    "            print(\"Notfound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run hashtags if first time running on machine\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def stopWordRemoval(data_frame):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = {}\n",
    "    for i in range(len(data_frame)):\n",
    "        word_tokens = word_tokenize(data_frame.iloc[i]['text'])\n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "        new_sentence = \" \".join(filtered_sentence)\n",
    "        filtered_text[data_frame.iloc[i]['text']] = new_sentence\n",
    "        #print(i, new_sentence)\n",
    "    data_frame[\"text\"].replace(filtered_text, inplace=True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, download\n",
    "# download('averaged_perceptron_tagger')\n",
    "# download('stopwords')\n",
    "# download('punkt')\n",
    "\n",
    "def POS_tagger(data_frame):\n",
    "    pos_data_tags = []\n",
    "    for i in range(len(data_frame)):\n",
    "        try:\n",
    "            word_tokens = word_tokenize(data_frame.iloc[i]['text'])\n",
    "        except:\n",
    "            print(data_frame.iloc[i])\n",
    "        pos = pos_tag(word_tokens)\n",
    "        exclusive_pos  = \" \".join(x[1] for x in pos)\n",
    "        pos_data_tags.append(exclusive_pos)\n",
    "        # pos_data_tags.append(pos)\n",
    "    data_frame.insert(1, \"pos\", pos_data_tags, True)\n",
    "    return data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "def misspelling_detection(data_frame):\n",
    "    spell = SpellChecker()\n",
    "    misspelled_count = []\n",
    "    for i in range(len(data_frame)):\n",
    "        word_list = word_tokenize(data_frame.iloc[i]['text'])\n",
    "        misspelled = spell.unknown(word_list)\n",
    "        misspelled_count.append(len(misspelled))\n",
    "        #print(i, len(misspelled), misspelled))\n",
    "    data_frame.insert(4, \"misspellings\", misspelled_count, True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "fake_words = defaultdict(lambda: 0)\n",
    "real_words = defaultdict(lambda: 0)\n",
    "total_words = defaultdict(lambda: 0)\n",
    "\n",
    "for i in range(len(df_train)):\n",
    "    text = str(df_train.iloc[i]['text']).lower()\n",
    "    text = text.split()\n",
    "    if(df_train.iloc[i]['label'] == 0):\n",
    "        for j in range(len(text)):\n",
    "            fake_words[text[j]] += 1\n",
    "            total_words[text[j]] += 1\n",
    "    else:\n",
    "        for j in range(len(text)):\n",
    "            real_words[text[j]] += 1\n",
    "            total_words[text[j]] += 1\n",
    "\n",
    "\n",
    "real_words = dict(sorted(real_words.items(), reverse=True, key=lambda item: item[1]))\n",
    "fake_words = dict(sorted(fake_words.items(), reverse=True, key=lambda item: item[1]))\n",
    "total_words = dict(sorted(total_words.items(), reverse=False, key=lambda item: item[1]))\n",
    "\n",
    "words = list(total_words.keys())\n",
    "for w in words:\n",
    "    if(total_words[w] < 100):\n",
    "        del total_words[w]\n",
    "        try:\n",
    "            del real_words[w]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            del fake_words[w]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "rw = Counter(real_words)\n",
    "fw = Counter(fake_words)\n",
    "fake_res = fw - rw\n",
    "real_res = rw - fw\n",
    "\n",
    "real_res = {k: real_res[k]/total_words[k] for k in total_words.keys() & real_res}\n",
    "fake_res = {k: fake_res[k]/total_words[k] for k in total_words.keys() & fake_res}\n",
    "\n",
    "real_res = dict(sorted(real_res.items(), reverse=True, key=lambda item: item[1]))\n",
    "fake_res = dict(sorted(fake_res.items(), reverse=True, key=lambda item: item[1]))\n",
    "\n",
    "\n",
    "sig_real_words = list(k for k, v in real_res.items() if v > 0.5)\n",
    "sig_fake_words = list(k for k, v in fake_res.items() if v > 0.5)\n",
    "\n",
    "def significance(data_frame, sig_words, clmn_name):\n",
    "    sig_found = []\n",
    "    for tweet in data_frame['text'].values:\n",
    "        try:\n",
    "            tweet = tweet.lower()\n",
    "            if(any(word in tweet for word in sig_words)):\n",
    "                sig_found.append(1)\n",
    "            else:\n",
    "                sig_found.append(0)\n",
    "        except:\n",
    "            sig_found.append(0)\n",
    "    data_frame.insert(4, clmn_name, sig_found, True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "def epochTime(data_frame):\n",
    "\n",
    "    month_dict = {\n",
    "        'Jan': '01',\n",
    "        'Feb': '02',\n",
    "        'Mar': '03',\n",
    "        'Apr': '04',\n",
    "        'May': '05',\n",
    "        'Jun': '06',\n",
    "        'Jul': '07',\n",
    "        'Aug': '08',\n",
    "        'Sep': '09',\n",
    "        'Oct': '10',\n",
    "        'Nov': '11',\n",
    "        'Dec': '12'\n",
    "    }\n",
    "    time_epoch = {}\n",
    "    for i in range(len(data_frame)):\n",
    "        str_time = str(data_frame.iloc[i]['timestamp'])\n",
    "        str_time = str_time.split()\n",
    "        for j in range(len(str_time)):\n",
    "            if(str_time[j] in month_dict.keys()):\n",
    "                str_time[j] = month_dict[str_time[j]]\n",
    "                break\n",
    "\n",
    "        str_time = str_time[2] + '.' + str_time[1] +'.' +str_time[len(str_time)-1]+' '+str_time[3][:2] + ':' +str_time[3][3:5] +':'+ str_time[3][6:]\n",
    "        str_time\n",
    "        try:\n",
    "            pattern = '%d.%m.%Y %H:%M:%S'\n",
    "            epoch = int(time.mktime(time.strptime(str_time, pattern)))\n",
    "        except:\n",
    "            epoch = -1\n",
    "        time_epoch[data_frame.iloc[i]['timestamp']] = epoch\n",
    "        \n",
    "    data_frame[\"timestamp\"].replace(time_epoch, inplace=True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def n_grams(data_frame, n):\n",
    "    a = []\n",
    "    for i in range(len(data_frame)):\n",
    "        sentence = data_frame.iloc[i]['text']\n",
    "\n",
    "        ng = ngrams(sentence.split(), n)\n",
    "        x = []\n",
    "        for grams in ng:\n",
    "            x.append(grams)\n",
    "        a.append(x)\n",
    "    data_frame.insert(3, ng+\"-gram\", a, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df):\n",
    "    # df = language_detect(df)\n",
    "    # print(\"Language Detection Done\")\n",
    "    # df = translate(df)\n",
    "    # print(\"Translation Done\")\n",
    "    # df = retweet_detection(df)\n",
    "    # print(\"Retweet Detection Done\")\n",
    "    df = news_companies(df)\n",
    "    print(\"News companies Detection Done\")\n",
    "    # df = numofhashtags(df)\n",
    "    # print(\"Hashtag count Done\")\n",
    "    # df = emoji_detect(df)\n",
    "    # print(\"Emoji count Done\")\n",
    "    # df = parseText(df)\n",
    "    # print(\"Parsing Done\")\n",
    "    # df = stopWordRemoval(df)\n",
    "    # print(\"Stopword removal Done\")\n",
    "    # df = misspelling_detection(df)\n",
    "    # print(\"Misspelling Count Done\")\n",
    "    # # df = profanity_detection(df)\n",
    "    # # print(\"Profanity Detection Done\")\n",
    "    # df = POS_tagger(df)\n",
    "    # print(\"POS tagging done\")\n",
    "    # df = significance(df, sig_real_words, \"significant_real\")\n",
    "    # print(\"Real Significant Words Detected\")\n",
    "    # df = significance(df, sig_fake_words, \"significant_fake\")\n",
    "    # print(\"Fake Significant Words Detected\")\n",
    "    # df = epochTime(df)\n",
    "    # print(\"Epoch Time Done\")\n",
    "\n",
    "    print(\"Preprocessing done\")\n",
    "    return df\n",
    "#before passing the text as unigram .lower everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misspelling Count Done\n",
      "Preprocessing done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>imageIds</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>misspellings</th>\n",
       "      <th>news_company_found</th>\n",
       "      <th>language</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>profane</th>\n",
       "      <th>retweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>remember movie day tomorrow reminds happening ...</td>\n",
       "      <td>VB NN NN NN VBZ VBG NN NN</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>1351550041</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>look sandy ny tremendous image hurricane looks...</td>\n",
       "      <td>NN JJ RB JJ NN NN VBZ IN NN NN CD JJ NN</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>1351537883</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good photo hurricane sandy reminds movie indep...</td>\n",
       "      <td>JJ NN NN JJ VBZ NN NN NN VBZ NN</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>1351534268</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scary shit hurricane ny</td>\n",
       "      <td>JJ VBD NN NN</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>1351538133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fave place world nyc hurricane sandy statueofl...</td>\n",
       "      <td>JJ NN NN NN NN NN NN</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>1351543562</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14478</th>\n",
       "      <td>slaps tweetdeck pigfish http</td>\n",
       "      <td>NNS VBP JJ NN</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14479</th>\n",
       "      <td>new species fish found brazil really good phot...</td>\n",
       "      <td>JJ NNS VBP VBN IN RB JJ NN VBP NN</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14480</th>\n",
       "      <td>call pigfish http</td>\n",
       "      <td>VB JJ NN</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14481</th>\n",
       "      <td>pigfish shark pork fish http co hqzwghydef</td>\n",
       "      <td>JJ NN NN JJ NN NN NN</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14482</th>\n",
       "      <td>decide fish meat pigfish http</td>\n",
       "      <td>RB JJ NN JJ NN</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14483 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      remember movie day tomorrow reminds happening ...   \n",
       "1      look sandy ny tremendous image hurricane looks...   \n",
       "2      good photo hurricane sandy reminds movie indep...   \n",
       "3                                scary shit hurricane ny   \n",
       "4      fave place world nyc hurricane sandy statueofl...   \n",
       "...                                                  ...   \n",
       "14478                       slaps tweetdeck pigfish http   \n",
       "14479  new species fish found brazil really good phot...   \n",
       "14480                                  call pigfish http   \n",
       "14481         pigfish shark pork fish http co hqzwghydef   \n",
       "14482                      decide fish meat pigfish http   \n",
       "\n",
       "                                           pos        imageIds   timestamp  \\\n",
       "0                    VB NN NN NN VBZ VBG NN NN  sandyA_fake_46  1351550041   \n",
       "1      NN JJ RB JJ NN NN VBZ IN NN NN CD JJ NN  sandyA_fake_09  1351537883   \n",
       "2              JJ NN NN JJ VBZ NN NN NN VBZ NN  sandyA_fake_09  1351534268   \n",
       "3                                 JJ VBD NN NN  sandyA_fake_29  1351538133   \n",
       "4                         JJ NN NN NN NN NN NN  sandyA_fake_15  1351543562   \n",
       "...                                        ...             ...         ...   \n",
       "14478                            NNS VBP JJ NN      pigFish_01          -1   \n",
       "14479        JJ NNS VBP VBN IN RB JJ NN VBP NN      pigFish_01          -1   \n",
       "14480                                 VB JJ NN      pigFish_01          -1   \n",
       "14481                     JJ NN NN JJ NN NN NN      pigFish_01          -1   \n",
       "14482                           RB JJ NN JJ NN      pigFish_01          -1   \n",
       "\n",
       "       misspellings  news_company_found language  num_emoji  num_hashtags  \\\n",
       "0                 0                   0       es          0             1   \n",
       "1                 1                   0       es          0             0   \n",
       "2                 1                   0       es          0             2   \n",
       "3                 0                   0       en          0             2   \n",
       "4                 2                   0       en          1             4   \n",
       "...             ...                 ...      ...        ...           ...   \n",
       "14478             3                   0       en          0             0   \n",
       "14479             1                   0       en          0             0   \n",
       "14480             2                   0       en          0             1   \n",
       "14481             3                   0       it          0             0   \n",
       "14482             2                   0       en          0             1   \n",
       "\n",
       "       profane  retweet  label  \n",
       "0            0        0      0  \n",
       "1            0        1      0  \n",
       "2            0        0      0  \n",
       "3            1        0      0  \n",
       "4            0        0      0  \n",
       "...        ...      ...    ...  \n",
       "14478        0        0      0  \n",
       "14479        0        0      0  \n",
       "14480        0        0      0  \n",
       "14481        0        0      0  \n",
       "14482        0        0      0  \n",
       "\n",
       "[14483 rows x 12 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pre_process(df_train)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News companies Detection Done\n",
      "Preprocessing done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>imageIds</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>news_company_found</th>\n",
       "      <th>language</th>\n",
       "      <th>profane</th>\n",
       "      <th>misspellings</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kereeen rt eclipse iss</td>\n",
       "      <td>JJ NN NN NN</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>1426844743</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>absolutely beautiful rt eclipse iss</td>\n",
       "      <td>RB JJ NN NN NN</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>1426849442</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eclipse iss 3 20 wow amazing</td>\n",
       "      <td>NN VBZ CD CD NN NN</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>1426853406</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eclipse iss</td>\n",
       "      <td>NN NN</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>1426842761</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eclipse seen iss something else divine creatio...</td>\n",
       "      <td>NN VBN JJ NN RB JJ NN NN</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>1426873451</td>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>zdf presenter confesses rigged video varoufaki...</td>\n",
       "      <td>NN NN NNS VBD NN NN NN JJ NN IN</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>1426744184</td>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>oh kleine liars zdf presenter confesses faked ...</td>\n",
       "      <td>UH NN NNS VBP NN NNS VBD JJ NN NN</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>1426744302</td>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>zdf program confirm varoufakis video montage</td>\n",
       "      <td>NN NN NN VBZ JJ NN</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>1426757001</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>11 34 almost noon big confusion varoufakis vid...</td>\n",
       "      <td>CD CD RB RB JJ NN NN NN NN JJ NN NN NN</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>1426761320</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>sorry english subtitles full english greek sub...</td>\n",
       "      <td>JJ JJ NNS JJ JJ NN NNS JJ RB VBP</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>1426713665</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3781 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0                                kereeen rt eclipse iss   \n",
       "1                   absolutely beautiful rt eclipse iss   \n",
       "2                          eclipse iss 3 20 wow amazing   \n",
       "3                                           eclipse iss   \n",
       "4     eclipse seen iss something else divine creatio...   \n",
       "...                                                 ...   \n",
       "3776  zdf presenter confesses rigged video varoufaki...   \n",
       "3777  oh kleine liars zdf presenter confesses faked ...   \n",
       "3778       zdf program confirm varoufakis video montage   \n",
       "3779  11 34 almost noon big confusion varoufakis vid...   \n",
       "3780  sorry english subtitles full english greek sub...   \n",
       "\n",
       "                                         pos      imageIds   timestamp  \\\n",
       "0                                JJ NN NN NN   eclipse_01   1426844743   \n",
       "1                             RB JJ NN NN NN   eclipse_01   1426849442   \n",
       "2                         NN VBZ CD CD NN NN   eclipse_01   1426853406   \n",
       "3                                      NN NN   eclipse_01   1426842761   \n",
       "4                   NN VBN JJ NN RB JJ NN NN    eclipse_01  1426873451   \n",
       "...                                      ...           ...         ...   \n",
       "3776         NN NN NNS VBD NN NN NN JJ NN IN  varoufakis_1  1426744184   \n",
       "3777       UH NN NNS VBP NN NNS VBD JJ NN NN  varoufakis_1  1426744302   \n",
       "3778                      NN NN NN VBZ JJ NN  varoufakis_1  1426757001   \n",
       "3779  CD CD RB RB JJ NN NN NN NN JJ NN NN NN  varoufakis_1  1426761320   \n",
       "3780        JJ JJ NNS JJ JJ NN NNS JJ RB VBP  varoufakis_1  1426713665   \n",
       "\n",
       "      news_company_found language  profane  misspellings  num_emoji  label  \n",
       "0                      0       en        0             2          0      0  \n",
       "1                      0       en        0             1          0      0  \n",
       "2                      0       en        0             0          0      0  \n",
       "3                      0       en        0             0          0      0  \n",
       "4                      0       fr        0             0          1      0  \n",
       "...                  ...      ...      ...           ...        ...    ...  \n",
       "3776                   0       fr        0             2          0      0  \n",
       "3777                   0       fr        0             2          0      0  \n",
       "3778                   0       es        0             2          0      0  \n",
       "3779                   0       de        0             4          2      0  \n",
       "3780                   0       en        0             1          0      0  \n",
       "\n",
       "[3781 rows x 10 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pre_process(df_test)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>imageIds</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>news_company_found</th>\n",
       "      <th>language</th>\n",
       "      <th>profane</th>\n",
       "      <th>misspellings</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kereeen rt eclipse iss</td>\n",
       "      <td>JJ NN NN NN</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>1426844743</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>absolutely beautiful rt eclipse iss</td>\n",
       "      <td>RB JJ NN NN NN</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>1426849442</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eclipse iss 3 20 wow amazing</td>\n",
       "      <td>NN VBZ CD CD NN NN</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>1426853406</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eclipse iss</td>\n",
       "      <td>NN NN</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>1426842761</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eclipse seen iss something else divine creatio...</td>\n",
       "      <td>NN VBN JJ NN RB JJ NN NN</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>1426873451</td>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>zdf presenter confesses rigged video varoufaki...</td>\n",
       "      <td>NN NN NNS VBD NN NN NN JJ NN IN</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>1426744184</td>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>oh kleine liars zdf presenter confesses faked ...</td>\n",
       "      <td>UH NN NNS VBP NN NNS VBD JJ NN NN</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>1426744302</td>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>zdf program confirm varoufakis video montage</td>\n",
       "      <td>NN NN NN VBZ JJ NN</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>1426757001</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>11 34 almost noon big confusion varoufakis vid...</td>\n",
       "      <td>CD CD RB RB JJ NN NN NN NN JJ NN NN NN</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>1426761320</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>sorry english subtitles full english greek sub...</td>\n",
       "      <td>JJ JJ NNS JJ JJ NN NNS JJ RB VBP</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>1426713665</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3781 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0                                kereeen rt eclipse iss   \n",
       "1                   absolutely beautiful rt eclipse iss   \n",
       "2                          eclipse iss 3 20 wow amazing   \n",
       "3                                           eclipse iss   \n",
       "4     eclipse seen iss something else divine creatio...   \n",
       "...                                                 ...   \n",
       "3776  zdf presenter confesses rigged video varoufaki...   \n",
       "3777  oh kleine liars zdf presenter confesses faked ...   \n",
       "3778       zdf program confirm varoufakis video montage   \n",
       "3779  11 34 almost noon big confusion varoufakis vid...   \n",
       "3780  sorry english subtitles full english greek sub...   \n",
       "\n",
       "                                         pos      imageIds   timestamp  \\\n",
       "0                                JJ NN NN NN   eclipse_01   1426844743   \n",
       "1                             RB JJ NN NN NN   eclipse_01   1426849442   \n",
       "2                         NN VBZ CD CD NN NN   eclipse_01   1426853406   \n",
       "3                                      NN NN   eclipse_01   1426842761   \n",
       "4                   NN VBN JJ NN RB JJ NN NN    eclipse_01  1426873451   \n",
       "...                                      ...           ...         ...   \n",
       "3776         NN NN NNS VBD NN NN NN JJ NN IN  varoufakis_1  1426744184   \n",
       "3777       UH NN NNS VBP NN NNS VBD JJ NN NN  varoufakis_1  1426744302   \n",
       "3778                      NN NN NN VBZ JJ NN  varoufakis_1  1426757001   \n",
       "3779  CD CD RB RB JJ NN NN NN NN JJ NN NN NN  varoufakis_1  1426761320   \n",
       "3780        JJ JJ NNS JJ JJ NN NNS JJ RB VBP  varoufakis_1  1426713665   \n",
       "\n",
       "      news_company_found language  profane  misspellings  num_emoji  label  \n",
       "0                      0       en        0             2          0      0  \n",
       "1                      0       en        0             1          0      0  \n",
       "2                      0       en        0             0          0      0  \n",
       "3                      0       en        0             0          0      0  \n",
       "4                      0       fr        0             0          1      0  \n",
       "...                  ...      ...      ...           ...        ...    ...  \n",
       "3776                   0       fr        0             2          0      0  \n",
       "3777                   0       fr        0             2          0      0  \n",
       "3778                   0       es        0             2          0      0  \n",
       "3779                   0       de        0             4          2      0  \n",
       "3780                   0       en        0             1          0      0  \n",
       "\n",
       "[3781 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_features(data_frame):\n",
    "    features = []\n",
    "    for i in range(len(data_frame)):\n",
    "        temp = []\n",
    "        #temp.append(data_frame.iloc[i]['significant_fake'])\n",
    "        #temp.append(data_frame.iloc[i]['significant_real'])\n",
    "        temp.append(data_frame.iloc[i]['profane'])\n",
    "        temp.append(data_frame.iloc[i]['misspellings'])\n",
    "        #temp.append(data_frame.iloc[i]['num_emoji'])\n",
    "        #temp.append(data_frame.iloc[i]['num_hashtags'])\n",
    "        temp.append(data_frame.iloc[i]['news_company_found'])\n",
    "        features.append(temp)\n",
    "    features = np.array(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_train['text']\n",
    "texts.append(df_test['text'])\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "df_train['text'] = tokenizer.texts_to_sequences(df_train['text'])\n",
    "MAX_TWEET_LENGTH = 16\n",
    "\n",
    "df_test['text'] = tokenizer.texts_to_sequences(df_test['text'])\n",
    "\n",
    "biggest_token = max(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = df_train.iloc[:]['text']\n",
    "X_test_text = df_test.iloc[:]['text']\n",
    "\n",
    "X_train_text = tf.keras.preprocessing.sequence.pad_sequences(X_train_text, maxlen=MAX_TWEET_LENGTH, padding='post', truncating='post')\n",
    "X_test_text = tf.keras.preprocessing.sequence.pad_sequences(X_test_text, maxlen=MAX_TWEET_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = pull_features(df_train)\n",
    "X_test_features = pull_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_features.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "features (InputLayer)           [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tweet_input (InputLayer)        [(None, 16)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feature_embedding (Embedding)   (None, 3, 50)        550         features[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "article_body_embedding (Embeddi (None, 16, 50)       454500      tweet_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "feature_conv (Conv1D)           (None, 1, 128)       19328       feature_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "article_body_conv (Conv1D)      (None, 9, 128)       51328       article_body_embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "feature_pooling (GlobalAverageP (None, 128)          0           feature_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "article_body_pooling (GlobalAve (None, 128)          0           article_body_conv[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           feature_pooling[0][0]            \n",
      "                                                                 article_body_pooling[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 75)           19275       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 50)           3800        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 20)           1020        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            21          dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 549,822\n",
      "Trainable params: 549,822\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feat_inputs = tf.keras.layers.Input(shape=(3,), name='features')\n",
    "feat_embed = tf.keras.layers.Embedding(X_train_features.max()+1, 50, input_length=3, name='feature_embedding')(feat_inputs)\n",
    "feat_conv = tf.keras.layers.Conv1D(128, 3, name='feature_conv')(feat_embed)\n",
    "feat_pool = tf.keras.layers.GlobalAveragePooling1D(name='feature_pooling')(feat_conv)\n",
    "\n",
    "\n",
    "text_input = tf.keras.layers.Input(shape=(MAX_TWEET_LENGTH,), name='tweet_input')\n",
    "text_embed = tf.keras.layers.Embedding(biggest_token + 1, 50, input_length=MAX_TWEET_LENGTH, name='article_body_embedding')(text_input)\n",
    "text_conv = tf.keras.layers.Conv1D(128, 8, name='article_body_conv')(text_embed)\n",
    "text_pool = tf.keras.layers.GlobalAveragePooling1D(name='article_body_pooling')(text_conv)\n",
    "\n",
    "concat = tf.keras.layers.concatenate([feat_pool, text_pool])\n",
    "dense_10 = tf.keras.layers.Dense(75, activation='selu')(concat)\n",
    "dense_5= tf.keras.layers.Dense(50, activation='selu')(dense_10)\n",
    "dense_2= tf.keras.layers.Dense(20, activation='selu')(dense_5)\n",
    "out_layer = tf.keras.layers.Dense(1, activation='sigmoid')(dense_2)\n",
    "model = tf.keras.models.Model(inputs=[feat_inputs, text_input], outputs=out_layer)\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14483 samples, validate on 3781 samples\n",
      "Epoch 1/100\n",
      "14483/14483 [==============================] - 2s 125us/sample - loss: 0.3740 - acc: 0.8271 - val_loss: 0.5166 - val_acc: 0.7903\n",
      "Epoch 2/100\n",
      "14483/14483 [==============================] - 2s 115us/sample - loss: 0.1742 - acc: 0.9323 - val_loss: 0.5401 - val_acc: 0.7982\n",
      "Epoch 3/100\n",
      "14483/14483 [==============================] - 2s 113us/sample - loss: 0.1216 - acc: 0.9553 - val_loss: 0.6049 - val_acc: 0.8373\n",
      "Epoch 4/100\n",
      "14483/14483 [==============================] - 2s 113us/sample - loss: 0.0982 - acc: 0.9639 - val_loss: 0.6729 - val_acc: 0.8228\n",
      "Epoch 5/100\n",
      "14483/14483 [==============================] - 2s 115us/sample - loss: 0.0856 - acc: 0.9673 - val_loss: 0.7783 - val_acc: 0.8146\n",
      "Epoch 6/100\n",
      "14483/14483 [==============================] - 2s 114us/sample - loss: 0.0796 - acc: 0.9698 - val_loss: 0.6948 - val_acc: 0.8471\n",
      "Epoch 7/100\n",
      "14483/14483 [==============================] - 2s 115us/sample - loss: 0.0746 - acc: 0.9721 - val_loss: 0.7233 - val_acc: 0.8477\n",
      "Epoch 8/100\n",
      "14483/14483 [==============================] - 2s 115us/sample - loss: 0.0671 - acc: 0.9744 - val_loss: 0.7744 - val_acc: 0.8448\n",
      "Epoch 9/100\n",
      "14483/14483 [==============================] - 2s 115us/sample - loss: 0.0651 - acc: 0.9757 - val_loss: 0.8293 - val_acc: 0.8350\n",
      "Epoch 10/100\n",
      "14483/14483 [==============================] - 2s 118us/sample - loss: 0.0623 - acc: 0.9762 - val_loss: 0.8871 - val_acc: 0.8474\n",
      "Epoch 11/100\n",
      "14483/14483 [==============================] - 2s 115us/sample - loss: 0.0571 - acc: 0.9779 - val_loss: 0.7705 - val_acc: 0.8514\n",
      "Epoch 12/100\n",
      "14483/14483 [==============================] - 2s 117us/sample - loss: 0.0559 - acc: 0.9782 - val_loss: 0.8902 - val_acc: 0.8434\n",
      "Epoch 13/100\n",
      "14483/14483 [==============================] - 2s 117us/sample - loss: 0.0555 - acc: 0.9782 - val_loss: 0.9496 - val_acc: 0.8413\n",
      "Epoch 14/100\n",
      "14483/14483 [==============================] - 2s 115us/sample - loss: 0.0525 - acc: 0.9787 - val_loss: 0.9588 - val_acc: 0.8408\n",
      "Epoch 15/100\n",
      "14483/14483 [==============================] - 2s 114us/sample - loss: 0.0498 - acc: 0.9809 - val_loss: 0.9860 - val_acc: 0.8416\n",
      "Epoch 16/100\n",
      "14483/14483 [==============================] - 2s 124us/sample - loss: 0.0491 - acc: 0.9807 - val_loss: 0.9747 - val_acc: 0.8479\n"
     ]
    }
   ],
   "source": [
    "y_train = df_train['label'].values\n",
    "y_test = df_test['label'].values\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, mode='max', restore_best_weights=True)\n",
    "\n",
    "history = model.fit([X_train_features, X_train_text], y_train, epochs=100, batch_size=128, validation_data=([X_test_features, X_test_text], y_test), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7705192790243767, 0.85136205]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8513620735255224"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.metrics as skl_m\n",
    "\n",
    "preds = model.predict([X_test_features, X_test_text])\n",
    "print(model.evaluate([X_test_features, X_test_text], y_test))\n",
    "\n",
    "y_pred = []\n",
    "for i in range(len(preds)):\n",
    "    if(preds[i]<0.5):\n",
    "        y_pred.append(0)\n",
    "    else:\n",
    "        y_pred.append(1)\n",
    "    \n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "f1 = skl_m.f1_score(y_test, y_pred, average='micro')\n",
    "f1\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6ac1c65944d4ad9c277d53219687cbb170d2c6377a780c0883c4434ac8bf052"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('coursework': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
