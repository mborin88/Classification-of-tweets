{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\borin\\anaconda3\\envs\\coursework\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#Tweets Machine Learning Coursework \n",
    "#COMP3222\n",
    "#\n",
    "\n",
    "# To support both python 2 and 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# These two lines are required to use Tensorflow 1\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# To plot nice figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Clear tensorflow's and reset seed\n",
    "def reset_graph(seed=None):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#get data\n",
    "def getData(file_name):\n",
    "    dict_data = {}\n",
    "    data_file = open(file_name, \"r\", encoding=\"utf8\")\n",
    "    raw_data_txt = data_file.readlines()\n",
    "    raw_data = []\n",
    "    for data in raw_data_txt:\n",
    "        raw_data.append(data.split(\"\\t\"))\n",
    "    raw_data.pop(0)\n",
    "\n",
    "    target_text = [data[6] for data in raw_data]\n",
    "    dict_data['tweetId'] = np.array([tweet[0] for tweet in raw_data])\n",
    "    dict_data['text'] = np.array([tweet[1] for tweet in raw_data])\n",
    "    dict_data['userId'] = np.array([tweet[2] for tweet in raw_data])\n",
    "    dict_data['imageIds'] = np.array([tweet[3] for tweet in raw_data])\n",
    "    dict_data['username'] = np.array([tweet[4] for tweet in raw_data])\n",
    "    dict_data['timestamp'] = np.array([tweet[5] for tweet in raw_data])\n",
    "    dict_data['label'] = np.array([1 if target=='real' or target =='real\\n' else 0 for target in target_text])\n",
    "\n",
    "    return dict_data\n",
    "\n",
    "df_train = pd.DataFrame.from_dict(getData(\"mediaeval-2015-trainingset.txt\"))\n",
    "df_test = pd.DataFrame.from_dict(getData(\"mediaeval-2015-testset.txt\"))\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_t_id(df):\n",
    "    check = ['.', ]\n",
    "    for i in range(len(df)):\n",
    "        x = str(df.iloc[i]['username'])\n",
    "        if(x.count('.')>0):\n",
    "            print(x)\n",
    "        \n",
    "\n",
    "scan_t_id(df_train)\n",
    "#scan_t_id(df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train_v2.csv\")\n",
    "df_train = df_train.iloc[:, 1:]\n",
    "df_train\n",
    "df_train.text = df_train.text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14460 ['Tue', 'Mar', '11', '23:', '59:', '22', '+0000', '2014']\n",
      "14461 ['Tue', 'Mar', '11', '13:', '57:', '22', '+0000', '2014']\n",
      "14462 ['Tue', 'Mar', '11', '03:', '13:', '40', '+0000', '2014']\n",
      "14463 ['Tue', 'Mar', '11', '03:', '13:', '40', '+0000', '2014']\n",
      "14464 ['Mon', 'Mar', '10', '22:', '00:', '13', '+0000', '2014']\n",
      "14465 ['Mon', 'Mar', '10', '21:', '47:', '50', '+0000', '2014']\n",
      "14466 ['Mon', 'Mar', '10', '21:', '21:', '02', '+0000', '2014']\n",
      "14467 ['Mon', 'Mar', '10', '19:', '32:', '38', '+0000', '2014']\n",
      "14468 ['Mon', 'Mar', '10', '14:', '11:', '23', '+0000', '2014']\n",
      "14469 ['Mon', 'Mar', '17', '11:', '46:', '36', '+0000', '2014']\n",
      "14470 ['Mon', 'Mar', '17', '08:', '54:', '26', '+0000', '2014']\n",
      "14471 ['Sun', 'Mar', '16', '03:', '48:', '40', '+0000', '2014']\n",
      "14472 ['Fri', 'Mar', '14', '17:', '31:', '06', '+0000', '2014']\n",
      "14473 ['Thu', 'Mar', '13', '20:', '07:', '05', '+0000', '2014']\n",
      "14474 ['Thu', 'Mar', '13', '02:', '37:', '05', '+0000', '2014']\n",
      "14475 ['Thu', 'Mar', '13', '00:', '15:', '44', '+0000', '2014']\n",
      "14476 ['Tue', 'Mar', '11', '16:', '06:', '41', '+0000', '2014']\n",
      "14477 ['Tue', 'Mar', '11', '12:', '38:', '51', '+0000', '2014']\n",
      "14478 ['Tue', 'Mar', '11', '03:', '48:', '36', '+0000', '2014']\n",
      "14479 ['Mon', 'Mar', '10', '18:', '09:', '26', '+0000', '2014']\n",
      "14480 ['Mon', 'Mar', '10', '10:', '59:', '45', '+0000', '2014']\n",
      "14481 ['Sun', 'Mar', '09', '20:', '07:', '10', '+0000', '2014']\n",
      "14482 ['Sun', 'Mar', '09', '16:', '36:', '09', '+0000', '2014']\n"
     ]
    }
   ],
   "source": [
    "def date(df):\n",
    "    for i in range(len(df)):\n",
    "        x = str(df.iloc[i]['timestamp'])\n",
    "        x = x.split(' ')\n",
    "        if(len(x) != 6):\n",
    "            print(i, x)\n",
    "        #print(x[3][3:5])\n",
    "date(df_train)\n",
    "# date(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my cousin sent this to me cleveland voice we gon die hurricane hurricanesandy sandy'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "x = \"My cousin sent this to me... :: Cleveland voice ::     We gon' die!!! ‚òÅ‚ö°‚òîüåä #hurricane #hurricanesandy #sandy http://t.co/XtW4exLn\"\n",
    "parsed = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split())\n",
    "parsed.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'rt'} ['it']\n",
      "2 {'id4'} ['id']\n",
      "4 {'nyc', 'statueofliberty'} ['ny', 'statueofliberty']\n",
      "5 {'nyc', '42nd'} ['ny', 'and']\n",
      "6 {'frankenstorm'} ['frankenstorm']\n",
      "8 {'newyork', 'statueofliberty'} ['network', 'statueofliberty']\n",
      "9 {'nyc'} ['ny']\n",
      "10 {'newyork', 'robertosalibaba'} ['network', 'robertosalibaba']\n",
      "12 {'newjersey'} ['newjersey']\n",
      "13 {'newyork'} ['network']\n",
      "16 {'newyork', 'bcs'} ['network', 'bus']\n",
      "17 {'nh', 'md', 'ct', 'nc', 'nj'} ['no', 'my', 'it', 'no', 'no']\n",
      "18 {'nyc', 'newyork'} ['ny', 'network']\n",
      "20 {'hurricanesandy', 'statueofliberty'} ['hurricanesandy', 'statueofliberty']\n",
      "22 {'hurricanesandy'} ['hurricanesandy']\n",
      "23 {'frankenstorm'} ['frankenstorm']\n",
      "25 {'newyork', 'picoftheday', 'statueofliberty'} ['network', 'picoftheday', 'statueofliberty']\n",
      "26 {'puupy'} ['puppy']\n",
      "27 {'nyc'} ['ny']\n",
      "28 {'nyc', 'jerseyithink', 'naturaldisasterornot', 'somethingisntright'} ['ny', 'jerseyithink', 'naturaldisasterornot', 'somethingisntright']\n",
      "30 {'nyc'} ['ny']\n",
      "32 {'nodaysoff', 'dayoff'} ['nodaysoff', 'payoff']\n",
      "33 {'newyork'} ['network']\n",
      "34 {'nyc', 'newyorkcity', 'ladyliberty'} ['ny', 'newyorkcity', 'ladyliberty']\n",
      "37 {'statueofliberty'} ['statueofliberty']\n",
      "41 {'ummmm', 'nj', 'woahh'} ['ummm', 'no', 'woah']\n",
      "43 {'landshark', 'nj', 's'} ['landmark', 'no', 'i']\n",
      "45 {'newyork'} ['network']\n",
      "46 {'damnnn', 'rp'} ['damn', 'up']\n",
      "47 {'newyork', 'instahub', 'instago', 'instaboy', 'instagood'} ['network', 'instahub', 'instant', 'instable', 'instagood']\n",
      "48 {'newyork'} ['network']\n",
      "49 {'nyc'} ['ny']\n",
      "50 {'newjersey', 'n'} ['newjersey', 'i']\n",
      "51 {'eastcoast'} ['eastcoast']\n",
      "52 {'bitchimashark', 's'} ['bitchimashark', 'i']\n",
      "53 {'nyc', 'statueofliberty'} ['ny', 'statueofliberty']\n",
      "56 {'nyc', 'newyork', 'ghostcity', 'timesquare'} ['ny', 'network', 'ghostcity', 'timeshare']\n",
      "57 {'newyork'} ['network']\n",
      "58 {'ilovecali'} ['ilovecali']\n",
      "60 {'nyc'} ['ny']\n",
      "61 {'nyc', 'rt'} ['ny', 'it']\n",
      "62 {'wtf', 'jetblue'} ['wif', 'jetblue']\n",
      "63 {'realimage', 'prayin', 'newyork', 'm'} ['reanimate', 'praying', 'network', 'i']\n",
      "64 {'nyc'} ['ny']\n",
      "65 {'newyork', 'newyorkcity', 'fuckingrun'} ['network', 'newyorkcity', 'fuckingrun']\n",
      "66 {'xl', 'osandy', 'proteja', 'furac', 'eua'} ['al', 'sandy', 'protect', 'fur', 'era']\n",
      "68 {'newyork', 'terrore'} ['network', 'terror']\n",
      "69 {'newyork'} ['network']\n",
      "70 {'frankenstorm'} ['frankenstorm']\n",
      "71 {'t', 'endoftheworld', 's'} ['i', 'endoftheworld', 'i']\n",
      "72 {'newjersey'} ['newjersey']\n",
      "73 {'statueofliberty'} ['statueofliberty']\n",
      "74 {'nyc', 'igersnewyork', 'newyork'} ['ny', 'igersnewyork', 'network']\n",
      "75 {'nofilter'} ['filter']\n",
      "76 {'newyork', 'helenanovellas'} ['network', 'helenanovellas']\n",
      "77 {'nyc'} ['ny']\n",
      "79 {'newyork', 's'} ['network', 'i']\n",
      "80 {'nyc'} ['ny']\n",
      "81 {'newyork'} ['network']\n",
      "82 {'nyc', 'laurenliithiium', 's'} ['ny', 'laurenliithiium', 'i']\n",
      "83 {'hurricanesandy', 'nofilter', 'instalikes', 's'} ['hurricanesandy', 'filter', 'instalikes', 'i']\n",
      "84 {'newyork'} ['network']\n",
      "85 {'newyork'} ['network']\n",
      "86 {'audhubillah', 'newyork'} ['audhubillah', 'network']\n",
      "87 {'nyc', 'newyorkharbor', 'statueofliberty', 's'} ['ny', 'newyorkharbor', 'statueofliberty', 'i']\n",
      "88 {'hurricanesandy'} ['hurricanesandy']\n",
      "89 {'nyc', 'id4', 'instagreat'} ['ny', 'id', 'instagram']\n",
      "92 {'nyc', 'statueofliberty', 'hurricanesandy', 'hudsonriver'} ['ny', 'statueofliberty', 'hurricanesandy', 'hudsonriver']\n",
      "93 {'thinkingofyou'} ['thinkingofyou']\n",
      "95 {'feest', 'newyork'} ['feet', 'network']\n",
      "98 {'reallife'} ['realize']\n",
      "100 {'instapic', 'instamood', 'nj', 'n'} ['instapic', 'instamood', 'no', 'i']\n",
      "103 {'frenkenstrom', 'nj', 's'} ['frenkenstrom', 'no', 'i']\n",
      "104 {'newyork', 'picoftheday', 'instagramers'} ['network', 'picoftheday', 'instagrams']\n",
      "106 {'nyc', 'newyork', 'statueofliberty'} ['ny', 'network', 'statueofliberty']\n",
      "107 {'j', 'n', 'mt'} ['i', 'i', 'it']\n",
      "110 {'w', 'newyork', 'instastorm', 'sbcdr'} ['i', 'network', 'instastorm', 'scar']\n",
      "111 {'nyc', 'crazyyy'} ['ny', 'crazy']\n",
      "112 {'eastcoast', 'staysafe', 'eyeofstorm'} ['eastcoast', 'staysail', 'eyeofstorm']\n",
      "114 {'nyc', 'timessquare', 'thatbitch', 'rescuingtrainrats', 'notreal', 'thisiscomical'} ['ny', 'timessquare', 'thatwith', 'rescuingtrainrats', 'normal', 'thisiscomical']\n",
      "115 {'nyc', 'newyork', 'frankenstorm', 'manhatten'} ['ny', 'network', 'frankenstorm', 'manhattan']\n",
      "116 {'newyork', '2nd'} ['network', 'and']\n",
      "117 {'eastcoast', 'newjersey'} ['eastcoast', 'newjersey']\n",
      "118 {'nyc', 'newyork', 'instadaily', 'ballz', 'instahub', 'instagood'} ['ny', 'network', 'instadaily', 'ball', 'instahub', 'instagood']\n",
      "119 {'newyork'} ['network']\n",
      "120 {'nodaysoff', 'tomboftheunknown', 'teamarmy'} ['nodaysoff', 'tomboftheunknown', 'teamarmy']\n",
      "121 {'nyc', 'newyork', 'statueof', 'photooftheday', 'newyorkcity'} ['ny', 'network', 'stateof', 'photooftheday', 'newyorkcity']\n",
      "122 {'nyc', 'newyorkharbor', 'statueofliberty', 's'} ['ny', 'newyorkharbor', 'statueofliberty', 'i']\n",
      "123 {'nyc', 'hurricanesandy'} ['ny', 'hurricanesandy']\n",
      "124 {'dc'} ['do']\n",
      "125 {'newjersey'} ['newjersey']\n",
      "126 {'hurricanesandy', 'scarry', 'igdaily'} ['hurricanesandy', 'carry', 'daily']\n",
      "129 {'reallife'} ['realize']\n",
      "130 {'dc', 'tomboftheunknowns'} ['do', 'tomboftheunknowns']\n",
      "131 {'staysafe'} ['staysail']\n",
      "132 {'wtf', '2012problems'} ['wif', '2012problems']\n",
      "133 {'nyc'} ['ny']\n",
      "134 {'usarmy', 'totus'} ['army', 'lotus']\n",
      "135 {'newyork'} ['network']\n",
      "136 {'nyc', 'rt'} ['ny', 'it']\n",
      "137 {'eastcoast', 'itagalot', 'bestwishes'} ['eastcoast', 'itagalot', 'bestwishes']\n",
      "138 {'nyc', 'rt'} ['ny', 'it']\n",
      "139 {'nyc'} ['ny']\n",
      "140 {'shittt'} ['shitty']\n",
      "141 {'nyc', 'newyorkcity', 'statueofliberty', 'newyork'} ['ny', 'newyorkcity', 'statueofliberty', 'network']\n",
      "143 {'newjersey'} ['newjersey']\n",
      "144 {'nyc', 'ghosttown'} ['ny', 'ghosttown']\n",
      "145 {'sikmats'} ['sikmats']\n",
      "146 {'nyc', 'newyork', 'libertystate', 'newyorkcity', 'instaphot'} ['ny', 'network', 'libertystate', 'newyorkcity', 'instaphot']\n",
      "148 {'nj'} ['no']\n",
      "149 {'t', 'nhttp', 'n', 'sxxc9iu6'} ['i', 'nutty', 'i', 'sxxc9iu6']\n",
      "150 {'photoftheday', 'newyorkcity'} ['photoftheday', 'newyorkcity']\n",
      "151 {'nofilter', 'newyork'} ['filter', 'network']\n",
      "152 {'nyc', 'newyork', 'frankenstorm', 'manhatten'} ['ny', 'network', 'frankenstorm', 'manhattan']\n",
      "153 {'hurricanesandy'} ['hurricanesandy']\n",
      "155 {'holyshit'} ['holoship']\n",
      "157 {'atlanticcity', 's'} ['atlanticcity', 'i']\n",
      "159 {'wtf', 'foh', 'newjersey', 'imout', 'crazyshit'} ['wif', 'for', 'newjersey', 'itout', 'crazyshit']\n",
      "162 {'newyork', 'n', 'frankenstorm', 'eastcoast', 'statueofliberty'} ['network', 'i', 'frankenstorm', 'eastcoast', 'statueofliberty']\n",
      "163 {'nofilter', 'instagod', 'nyc', 'skyporn'} ['filter', 'instated', 'ny', 'skyporn']\n",
      "166 {'hurricanesandy', 'rt', 'instadaily'} ['hurricanesandy', 'it', 'instadaily']\n",
      "167 {'nyc', 'n'} ['ny', 'i']\n",
      "168 {'newyork'} ['network']\n",
      "170 {'nyc', 'newyork', 'iphonesia', 'igers', 's', 'picoftheday'} ['ny', 'network', 'iphones', 'tigers', 'i', 'picoftheday']\n",
      "171 {'whylin', 'wfr', 'n'} ['whaling', 'war', 'i']\n",
      "172 {'newjersey'} ['newjersey']\n",
      "173 {'newyork', 'staysafe'} ['network', 'staysail']\n",
      "174 {'impressionante', 'instapic', 'instagood', 'hurricanesandy', 'furac', 'eua'} ['impressionable', 'instapic', 'instagood', 'hurricanesandy', 'fur', 'era']\n",
      "176 {'nlooks'} ['looks']\n",
      "178 {'t', 'newyork', 'frankinstorm', 'statueofliberty'} ['i', 'network', 'frankinstorm', 'statueofliberty']\n",
      "179 {'eusoualenda', 'newyork', 'iamlegend'} ['eusoualenda', 'network', 'iamlegend']\n",
      "180 {'newyork'} ['network']\n",
      "181 {'dc', 'tomboftheunknownsoldier'} ['do', 'tomboftheunknownsoldier']\n",
      "182 {'nyc', 'eastcoast', 'statueofliberty'} ['ny', 'eastcoast', 'statueofliberty']\n",
      "183 {'nyc', 'instabest', 'instashot', 'instapic', 'instacool', 'instada'} ['ny', 'instabest', 'instashot', 'instapic', 'instacool', 'instead']\n",
      "185 {'newyork', 'weatherporn'} ['network', 'weatherporn']\n",
      "186 {'nyc', 'n', 'proffisional'} ['ny', 'i', 'proffesional']\n",
      "187 {'newyork'} ['network']\n",
      "188 {'newyork', 'like4like'} ['network', 'lifelike']\n",
      "190 {'nyc', 'vs'} ['ny', 'is']\n",
      "191 {'hurricanesandy', 'eastcoas'} ['hurricanesandy', 'eastcoas']\n",
      "192 {'nyc', 'n', 'unitedstates', 'instago', 'instame'} ['ny', 'i', 'unitedstates', 'instant', 'instate']\n",
      "193 {'hurricanesandy', 'n', 'statueofliberty'} ['hurricanesandy', 'i', 'statueofliberty']\n",
      "194 {'newyork', 'byever'} ['network', 'bever']\n",
      "195 {'nyc'} ['ny']\n",
      "196 {'nyc', 'eastcoast', 'nj'} ['ny', 'eastcoast', 'no']\n",
      "197 {'t', 'newyork', 'statueofliberty'} ['i', 'network', 'statueofliberty']\n",
      "198 {'prayfornyc', 'eastcoast', 'iloveny'} ['prayfornyc', 'eastcoast', 'lovely']\n",
      "199 {'eastcoast'} ['eastcoast']\n",
      "200 {'rt', 'wtf'} ['it', 'wif']\n",
      "201 {'eeuu', 'iphonesia'} ['menu', 'iphones']\n",
      "202 {'newyork'} ['network']\n",
      "204 {'nyc', 'instamood', 'instalove', 'instagood'} ['ny', 'instamood', 'instalove', 'instagood']\n",
      "206 {'nyc', 'hurricanesandy', 'n', 'instamazing', 'instagreat'} ['ny', 'hurricanesandy', 'i', 'instamazing', 'instagram']\n",
      "209 {'newyork'} ['network']\n",
      "210 {'hurricanesandy', 'atlanticcity', 'newjersey', 'nj'} ['hurricanesandy', 'atlanticcity', 'newjersey', 'no']\n",
      "211 {'statueliberty'} ['statueliberty']\n",
      "212 {'newyork', 'subhanallah'} ['network', 'subhanallah']\n",
      "213 {'newyork', 'n'} ['network', 'i']\n",
      "214 {'newjersey', 'nj', 's'} ['newjersey', 'no', 'i']\n",
      "215 {'praysandthoughts'} ['praysandthoughts']\n",
      "216 {'newyork', 'statueofliberty', 'like4like', 'follow4follow'} ['network', 'statueofliberty', 'lifelike', 'follow4follow']\n",
      "218 {'newyork'} ['network']\n",
      "219 {'nyc'} ['ny']\n",
      "220 {'newyork'} ['network']\n",
      "222 {'frankenstorm', 'newyork'} ['frankenstorm', 'network']\n",
      "223 {'nyc', 'iwn'} ['ny', 'in']\n",
      "224 {'milit', 's'} ['milt', 'i']\n",
      "225 {'newyork'} ['network']\n",
      "226 {'nyc'} ['ny']\n",
      "227 {'newyork', 'picoftheday'} ['network', 'picoftheday']\n",
      "228 {'nyc', 'godbless'} ['ny', 'godless']\n",
      "229 {'newyork'} ['network']\n",
      "230 {'nyc', 'likeback', 'instadaily', 'like4like', 'instaphoto'} ['ny', 'likeback', 'instadaily', 'lifelike', 'instaphoto']\n",
      "231 {'s', 'unknownsolders', 'trueheros'} ['i', 'unknownsolders', 'truthers']\n",
      "232 {'statueofliberty'} ['statueofliberty']\n",
      "233 {'newyork'} ['network']\n",
      "234 {'safetyfirst'} ['safetyfirst']\n",
      "235 {'s'} ['i']\n",
      "236 {'nyc', 'newyork', 'gohomesandy', 'vs', 'eastcoast'} ['ny', 'network', 'gohomesandy', 'is', 'eastcoast']\n",
      "237 {'newyork', 'statueofliberty'} ['network', 'statueofliberty']\n",
      "238 {'newyork'} ['network']\n",
      "239 {'statueofliberty'} ['statueofliberty']\n",
      "241 {'nyc', 'timessquare'} ['ny', 'timessquare']\n",
      "242 {'nyc', 'newyork', 'thatbitchsandy', 'thoughtsandprayers', 'statueofliberty'} ['ny', 'network', 'thatbitchsandy', 'thoughtsandprayers', 'statueofliberty']\n",
      "247 {'oct29', 'n', 'newyorkers'} ['oct', 'i', 'newyorkers']\n",
      "248 {'newyork', 'hahah'} ['network', 'haha']\n",
      "250 {'hurricanesandy', 'atlanticcity', 'nj', 's'} ['hurricanesandy', 'atlanticcity', 'no', 'i']\n",
      "251 {'regram', 's'} ['regret', 'i']\n",
      "252 {'nj'} ['no']\n",
      "254 {'newyorkcity', 'newyork'} ['newyorkcity', 'network']\n",
      "255 {'newyork', 'eastcoast'} ['network', 'eastcoast']\n",
      "256 {'mysandy', 'wtf'} ['sandy', 'wif']\n",
      "257 {'staysafenj'} ['staysafenj']\n",
      "259 {'sandydc'} ['sandy']\n",
      "264 {'nyc', 'nj', 'statueofliberty'} ['ny', 'no', 'statueofliberty']\n",
      "265 {'newyork'} ['network']\n",
      "266 {'nyc', 'rt', 'statueofliberty', 'newyork'} ['ny', 'it', 'statueofliberty', 'network']\n",
      "267 {'shyt'} ['shut']\n",
      "268 {'newyork'} ['network']\n",
      "270 {'onpost', 'unknownsoldier'} ['unpost', 'unknownsoldier']\n",
      "271 {'nyc'} ['ny']\n",
      "272 {'nyc', 'timessquare', 'hurricanesandy'} ['ny', 'timessquare', 'hurricanesandy']\n",
      "273 {'newyork'} ['network']\n",
      "274 {'nyc', 'newyork'} ['ny', 'network']\n",
      "275 {'staystrong', 'tomboftheunknown'} ['staystrong', 'tomboftheunknown']\n",
      "276 {'j', 'qt', 'n', 'mmm', 'k'} ['i', 'it', 'i', 'mom', 'i']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20824/90690309.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mcorrections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmisspelled_test\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mcorrections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmisspelled_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmisspelled_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrections\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\coursework\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36mcorrection\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    177\u001b[0m                 str: The most likely candidate \"\"\"\n\u001b[0;32m    178\u001b[0m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[0mcandidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\coursework\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36mcandidates\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;31m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distance\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__edit_distance_alt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\coursework\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36m__edit_distance_alt\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_if_should_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         ]\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0me2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medit_distance_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_if_should_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\coursework\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_if_should_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         ]\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0me2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medit_distance_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_if_should_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\coursework\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36mknown\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    216\u001b[0m                 \u001b[0mset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mset\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthose\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mare\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                 corpus \"\"\"\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mensure_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_case_sensitive\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         return set(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\coursework\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    216\u001b[0m                 \u001b[0mset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mset\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthose\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mare\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                 corpus \"\"\"\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mensure_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_case_sensitive\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         return set(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Spell checker test code\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "for i in range(len(df_train)):\n",
    "    test = df_train.iloc[i]['text']\n",
    "    test = test.split(\" \")\n",
    "\n",
    "    # find those words that may be misspelled\n",
    "    misspelled_test = spell.unknown(test)\n",
    "    misspelling_found_test = False\n",
    "    misspell_count_test = 0\n",
    "    corrections = []\n",
    "    for word in misspelled_test:\n",
    "        corrections.append(spell.correction(word))\n",
    "    if(len(misspelled_test)>0):\n",
    "        print(i, misspelled_test, corrections)\n",
    "\n",
    "\n",
    "# print(len(misspelled_test))\n",
    "# print(test)\n",
    "# print(misspelled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liver un facebook para cuando andas pedo http t co 1sivd33y1j http t co cP63lVuAz8\n"
     ]
    }
   ],
   "source": [
    "x = str(\"Liver un facebook para cuando andas pedo http: \\/\\/t.co\\/1sivd33y1j http: \\/\\/t.co\\/cP63lVuAz8\")\n",
    "parsed = ' '.join(re.sub(\"(@[A-Za-z0-9_]+)|((\\\\\\/\\\\\\/\\w+.co\\S+)|(\\w+:\\/\\/\\S+))|([^0-9A-Za-z \\t])\",\" \",x).split())\n",
    "parsed = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split())\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.34186893933428164\n",
      "  (0, 8)\t0.34186893933428164\n",
      "  (0, 9)\t0.24324256601037686\n",
      "  (0, 7)\t0.24324256601037686\n",
      "  (0, 0)\t0.24324256601037686\n",
      "  (0, 4)\t0.24324256601037686\n",
      "  (0, 3)\t0.4864851320207537\n",
      "  (0, 6)\t0.34186893933428164\n",
      "  (0, 5)\t0.34186893933428164\n",
      "  (0, 2)\t0.24324256601037686\n",
      "  (1, 10)\t0.3759577127469508\n",
      "  (1, 9)\t0.26749700905276197\n",
      "  (1, 7)\t0.5349940181055239\n",
      "  (1, 0)\t0.26749700905276197\n",
      "  (1, 4)\t0.26749700905276197\n",
      "  (1, 3)\t0.26749700905276197\n",
      "  (1, 2)\t0.5349940181055239\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "x = \"Hello my name is Michael and the weather is very fine\"\n",
    "y = \"Hello the word is Michael and the weather Hello\"\n",
    "\n",
    "v = TfidfVectorizer()\n",
    "response = v.fit_transform([x, y])\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'fine', 'hello', 'is', 'michael', 'my', 'name', 'the', 'very', 'weather', 'word']\n"
     ]
    }
   ],
   "source": [
    "print(v.get_feature_names())\n",
    "token_id = list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train_v2.2.csv\")\n",
    "df_train = df_train.iloc[:, 1:]\n",
    "df_train\n",
    "df_train.text = df_train.text.astype(str)\n",
    "df_train.pos = df_train.pos.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "POS_tags_fake = defaultdict(lambda: 0)\n",
    "POS_tags_real= defaultdict(lambda: 0)\n",
    "POS_tags_total= defaultdict(lambda: 0)\n",
    "\n",
    "for i in range(len(df_train)):\n",
    "    text = str(df_train.iloc[i]['pos'])\n",
    "    text = text.split()\n",
    "    if(df_train.iloc[i]['label'] == 0):\n",
    "        for j in range(len(text)):\n",
    "            POS_tags_fake[text[j]] += 1\n",
    "            POS_tags_total[text[j]] += 1\n",
    "    else:\n",
    "        for j in range(len(text)):\n",
    "            POS_tags_real[text[j]] += 1\n",
    "            POS_tags_total[text[j]] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_tags_fake = dict(sorted(POS_tags_fake.items(), reverse=True, key=lambda item: item[1]))\n",
    "POS_tags_real = dict(sorted(POS_tags_real.items(), reverse=True, key=lambda item: item[1]))\n",
    "POS_tags_total = dict(sorted(POS_tags_total.items(), reverse=False, key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'VBD': 0.6552714737144498,\n",
       "  'NNS': 0.6433876221498371,\n",
       "  'RB': 0.6416107382550336,\n",
       "  'VB': 0.6401617250673854,\n",
       "  'VBZ': 0.636930754834685,\n",
       "  'JJ': 0.6333138079769955,\n",
       "  'NN': 0.6290381052122774,\n",
       "  'VBN': 0.6207928197456993,\n",
       "  'VBP': 0.6192210703278229,\n",
       "  'VBG': 0.6023040320560982,\n",
       "  'IN': 0.5735981308411215,\n",
       "  'CD': 0.5361179361179361},\n",
       " {'CD': 0.4638820638820639,\n",
       "  'IN': 0.4264018691588785,\n",
       "  'VBG': 0.39769596794390183,\n",
       "  'VBP': 0.3807789296721771,\n",
       "  'VBN': 0.37920718025430067,\n",
       "  'NN': 0.37096189478772257,\n",
       "  'JJ': 0.3666861920230044,\n",
       "  'VBZ': 0.363069245165315,\n",
       "  'VB': 0.35983827493261455,\n",
       "  'RB': 0.35838926174496644,\n",
       "  'NNS': 0.35661237785016286,\n",
       "  'VBD': 0.3447285262855501})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "rw = Counter(POS_tags_real)\n",
    "fw = Counter(POS_tags_fake)\n",
    "# fw = fw - rw\n",
    "# rw = rw - fw\n",
    "POS_tags_total = {key:val for key, val in POS_tags_total.items() if val > 500}\n",
    "rw = {k: rw[k]/POS_tags_total[k] for k in POS_tags_total.keys() & rw}\n",
    "fw = {k: fw[k]/POS_tags_total[k] for k in POS_tags_total.keys() & fw}\n",
    "\n",
    "rw = dict(sorted(rw.items(), reverse=True, key=lambda item: item[1]))\n",
    "fw = dict(sorted(fw.items(), reverse=True, key=lambda item: item[1]))\n",
    "fw, rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_tags_total['FW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'this', 'foo', 'bar', 'sentences')\n",
      "('is', 'this', 'foo', 'bar', 'sentences', 'and')\n",
      "('this', 'foo', 'bar', 'sentences', 'and', 'i')\n",
      "('foo', 'bar', 'sentences', 'and', 'i', 'want')\n",
      "('bar', 'sentences', 'and', 'i', 'want', 'to')\n",
      "('sentences', 'and', 'i', 'want', 'to', 'ngramize')\n",
      "('and', 'i', 'want', 'to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def n_grams(data_frame, n):\n",
    "    a = []\n",
    "    for i in range(len(data_frame)):\n",
    "        sentence = data_frame.iloc[i]['text']\n",
    "\n",
    "        ng = ngrams(sentence.split(), n)\n",
    "        x = []\n",
    "        for grams in ng:\n",
    "            x.append(grams)\n",
    "        a.append(x)\n",
    "    data_frame.insert(3, ng+\"-gram\", a, True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6ac1c65944d4ad9c277d53219687cbb170d2c6377a780c0883c4434ac8bf052"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('coursework': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
