{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\borin\\anaconda3\\envs\\coursework\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#Tweets Machine Learning Coursework \n",
    "#COMP3222\n",
    "#\n",
    "\n",
    "# To support both python 2 and 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# These two lines are required to use Tensorflow 1\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# To plot nice figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Clear tensorflow's and reset seed\n",
    "def reset_graph(seed=None):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#get data\n",
    "def getData(file_name):\n",
    "    dict_data = {}\n",
    "    data_file = open(file_name, \"r\", encoding=\"utf8\")\n",
    "    raw_data_txt = data_file.readlines()\n",
    "    raw_data = []\n",
    "    for data in raw_data_txt:\n",
    "        raw_data.append(data.split(\"\\t\"))\n",
    "    raw_data.pop(0)\n",
    "\n",
    "    target_text = [data[6] for data in raw_data]\n",
    "    dict_data['text'] = np.array([tweet[1] for tweet in raw_data])\n",
    "    dict_data['imageIds'] = np.array([tweet[3] for tweet in raw_data])\n",
    "    dict_data['timestamp'] = np.array([tweet[5] for tweet in raw_data])\n",
    "    dict_data['label'] = np.array([1 if target=='real' or target =='real\\n' else 0 for target in target_text])\n",
    "\n",
    "    return dict_data\n",
    "\n",
    "df_train = pd.DataFrame.from_dict(getData(\"mediaeval-2015-trainingset.txt\"))\n",
    "df_test = pd.DataFrame.from_dict(getData(\"mediaeval-2015-testset.txt\"))\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>imageIds</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿Se acuerdan de la película: “El día después d...</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda i...</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buena la foto del Huracán Sandy, me recuerda a...</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My fave place in the world #nyc #hurricane #sa...</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14478</th>\n",
       "      <td>@BobombDom *slaps TweetDeck with the PigFish h...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Tue Mar 11 03: 48: 36 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14479</th>\n",
       "      <td>New Species of Fish found in Brazil or just Re...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Mon Mar 10 18: 09: 26 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14480</th>\n",
       "      <td>What do we call this? #pigFISH http: \\/\\/t.co\\...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Mon Mar 10 10: 59: 45 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14481</th>\n",
       "      <td>Pigfish ? E dopo il pescecane c'è il pesce mai...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Sun Mar 09 20: 07: 10 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14482</th>\n",
       "      <td>For those who can't decide between fish or mea...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Sun Mar 09 16: 36: 09 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14483 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text        imageIds  \\\n",
       "0      ¿Se acuerdan de la película: “El día después d...  sandyA_fake_46   \n",
       "1      @milenagimon: Miren a Sandy en NY!  Tremenda i...  sandyA_fake_09   \n",
       "2      Buena la foto del Huracán Sandy, me recuerda a...  sandyA_fake_09   \n",
       "3         Scary shit #hurricane #NY http://t.co/e4JLBUfH  sandyA_fake_29   \n",
       "4      My fave place in the world #nyc #hurricane #sa...  sandyA_fake_15   \n",
       "...                                                  ...             ...   \n",
       "14478  @BobombDom *slaps TweetDeck with the PigFish h...      pigFish_01   \n",
       "14479  New Species of Fish found in Brazil or just Re...      pigFish_01   \n",
       "14480  What do we call this? #pigFISH http: \\/\\/t.co\\...      pigFish_01   \n",
       "14481  Pigfish ? E dopo il pescecane c'è il pesce mai...      pigFish_01   \n",
       "14482  For those who can't decide between fish or mea...      pigFish_01   \n",
       "\n",
       "                              timestamp  label  \n",
       "0        Mon Oct 29 22:34:01 +0000 2012      0  \n",
       "1        Mon Oct 29 19:11:23 +0000 2012      0  \n",
       "2        Mon Oct 29 18:11:08 +0000 2012      0  \n",
       "3        Mon Oct 29 19:15:33 +0000 2012      0  \n",
       "4        Mon Oct 29 20:46:02 +0000 2012      0  \n",
       "...                                 ...    ...  \n",
       "14478  Tue Mar 11 03: 48: 36 +0000 2014      0  \n",
       "14479  Mon Mar 10 18: 09: 26 +0000 2014      0  \n",
       "14480  Mon Mar 10 10: 59: 45 +0000 2014      0  \n",
       "14481  Sun Mar 09 20: 07: 10 +0000 2014      0  \n",
       "14482  Sun Mar 09 16: 36: 09 +0000 2014      0  \n",
       "\n",
       "[14483 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to run get data always before re running this one\n",
    "#tokenize train data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df_train['text'])\n",
    "df_train['text'] = tokenizer.texts_to_sequences(df_train['text'])\n",
    "max_tokens = map(max, df_train['text'].values[:])\n",
    "biggest_token = max(max_tokens)\n",
    "MAX_TWEET_LENGTH = 30\n",
    "\n",
    "#tokenize test data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df_test['text'])\n",
    "df_test['text'] = tokenizer.texts_to_sequences(df_test['text'])\n",
    "MAX_TWEET_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>imageIds</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[7198, 7199, 16, 23, 809, 2719, 1108, 783, 16,...</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4378, 1165, 9, 4, 39, 25, 4379, 165, 72, 48, ...</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[833, 23, 66, 72, 48, 4, 90, 4377, 9, 23, 809,...</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[137, 84, 6, 25, 3, 1, 2, 7203]</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[38, 4381, 479, 8, 5, 252, 15, 6, 4, 102, 869,...</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14478</th>\n",
       "      <td>[28398, 28399, 28400, 71, 5, 1489, 3, 1, 2, 28...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Tue Mar 11 03: 48: 36 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14479</th>\n",
       "      <td>[14, 7197, 7, 2379, 92, 8, 28402, 282, 80, 220...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Mon Mar 10 18: 09: 26 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14480</th>\n",
       "      <td>[89, 156, 107, 644, 10, 1489, 3, 1, 2, 28405]</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Mon Mar 10 10: 59: 45 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14481</th>\n",
       "      <td>[1489, 402, 2955, 1770, 28406, 28407, 1770, 28...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Sun Mar 09 20: 07: 10 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14482</th>\n",
       "      <td>[22, 151, 105, 295, 2342, 2682, 2379, 282, 284...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Sun Mar 09 16: 36: 09 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14483 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text        imageIds  \\\n",
       "0      [7198, 7199, 16, 23, 809, 2719, 1108, 783, 16,...  sandyA_fake_46   \n",
       "1      [4378, 1165, 9, 4, 39, 25, 4379, 165, 72, 48, ...  sandyA_fake_09   \n",
       "2      [833, 23, 66, 72, 48, 4, 90, 4377, 9, 23, 809,...  sandyA_fake_09   \n",
       "3                        [137, 84, 6, 25, 3, 1, 2, 7203]  sandyA_fake_29   \n",
       "4      [38, 4381, 479, 8, 5, 252, 15, 6, 4, 102, 869,...  sandyA_fake_15   \n",
       "...                                                  ...             ...   \n",
       "14478  [28398, 28399, 28400, 71, 5, 1489, 3, 1, 2, 28...      pigFish_01   \n",
       "14479  [14, 7197, 7, 2379, 92, 8, 28402, 282, 80, 220...      pigFish_01   \n",
       "14480      [89, 156, 107, 644, 10, 1489, 3, 1, 2, 28405]      pigFish_01   \n",
       "14481  [1489, 402, 2955, 1770, 28406, 28407, 1770, 28...      pigFish_01   \n",
       "14482  [22, 151, 105, 295, 2342, 2682, 2379, 282, 284...      pigFish_01   \n",
       "\n",
       "                              timestamp  label  \n",
       "0        Mon Oct 29 22:34:01 +0000 2012      0  \n",
       "1        Mon Oct 29 19:11:23 +0000 2012      0  \n",
       "2        Mon Oct 29 18:11:08 +0000 2012      0  \n",
       "3        Mon Oct 29 19:15:33 +0000 2012      0  \n",
       "4        Mon Oct 29 20:46:02 +0000 2012      0  \n",
       "...                                 ...    ...  \n",
       "14478  Tue Mar 11 03: 48: 36 +0000 2014      0  \n",
       "14479  Mon Mar 10 18: 09: 26 +0000 2014      0  \n",
       "14480  Mon Mar 10 10: 59: 45 +0000 2014      0  \n",
       "14481  Sun Mar 09 20: 07: 10 +0000 2014      0  \n",
       "14482  Sun Mar 09 16: 36: 09 +0000 2014      0  \n",
       "\n",
       "[14483 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4588/803328775.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Find out word count of texts.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_lens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_lens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_lens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_lens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4588/803328775.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Find out word count of texts.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_lens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_lens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_lens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_lens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "#Find out word count of texts.\n",
    "X_lens = [len(x.split(' ')) for x in df_train['text'].values]\n",
    "X_lens = np.array(X_lens)\n",
    "plt.hist(X_lens, 50)\n",
    "plt.show()\n",
    "x = np.unique((X_lens >= MAX_TWEET_LENGTH), return_counts=True)\n",
    "#Length most under 30 in length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = tf.keras.preprocessing.sequence.pad_sequences(df_train['text'].values, maxlen=MAX_TWEET_LENGTH, padding='post', truncating='post')\n",
    "X_test_text = tf.keras.preprocessing.sequence.pad_sequences(df_test['text'].values, maxlen=MAX_TWEET_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14483, 30)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\borin\\anaconda3\\envs\\coursework\\lib\\site-packages\\tensorflow\\python\\keras\\initializers\\initializers_v1.py:58: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "article_body_input (InputLay [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "article_body_embedding (Embe (None, 30, 50)            1420650   \n",
      "_________________________________________________________________\n",
      "article_body_conv (Conv1D)   (None, 21, 256)           128256    \n",
      "_________________________________________________________________\n",
      "article_body_pooling (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,579,707\n",
      "Trainable params: 1,579,707\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text_input = tf.keras.layers.Input(shape=(MAX_TWEET_LENGTH,), name='article_body_input')\n",
    "text_embed = tf.keras.layers.Embedding(biggest_token + 1, 50, input_length=MAX_TWEET_LENGTH, name='article_body_embedding')(text_input)\n",
    "text_conv = tf.keras.layers.Conv1D(256, 10, name='article_body_conv')(text_embed)\n",
    "text_pool = tf.keras.layers.GlobalMaxPool1D(name='article_body_pooling')(text_conv)\n",
    "#concat = tf.keras.layers.concatenate([text_pool])\n",
    "dense_100 = tf.keras.layers.Dense(100, activation='relu')(text_pool)\n",
    "dense_50 = tf.keras.layers.Dense(50, activation='relu')(dense_100)\n",
    "out_layer = tf.keras.layers.Dense(1, activation='sigmoid')(dense_50)\n",
    "model = tf.keras.models.Model(inputs=[text_input], outputs=out_layer)\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14483 samples, validate on 3781 samples\n",
      "Epoch 1/100\n",
      "14464/14483 [============================>.] - ETA: 0s - loss: 0.5082 - acc: 0.7436WARNING:tensorflow:From C:\\Users\\borin\\anaconda3\\envs\\coursework\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "14483/14483 [==============================] - 5s 341us/sample - loss: 0.5078 - acc: 0.7438 - val_loss: 0.6439 - val_acc: 0.7249\n",
      "Epoch 2/100\n",
      "14483/14483 [==============================] - 5s 343us/sample - loss: 0.1652 - acc: 0.9327 - val_loss: 0.8939 - val_acc: 0.6414\n",
      "Epoch 3/100\n",
      "14483/14483 [==============================] - 5s 344us/sample - loss: 0.0493 - acc: 0.9838 - val_loss: 1.6173 - val_acc: 0.6133\n",
      "Epoch 4/100\n",
      "14483/14483 [==============================] - 5s 346us/sample - loss: 0.0061 - acc: 0.9987 - val_loss: 1.9758 - val_acc: 0.6334\n",
      "Epoch 5/100\n",
      "14483/14483 [==============================] - 5s 350us/sample - loss: 9.3695e-04 - acc: 0.9999 - val_loss: 2.1353 - val_acc: 0.6313\n",
      "Epoch 6/100\n",
      "14483/14483 [==============================] - 5s 349us/sample - loss: 7.7641e-04 - acc: 0.9999 - val_loss: 2.1791 - val_acc: 0.6112\n",
      "Epoch 7/100\n",
      "14483/14483 [==============================] - 5s 364us/sample - loss: 6.1871e-04 - acc: 0.9999 - val_loss: 2.5619 - val_acc: 0.6445\n",
      "Epoch 8/100\n",
      "14483/14483 [==============================] - 5s 374us/sample - loss: 0.0011 - acc: 0.9999 - val_loss: 2.4791 - val_acc: 0.6374\n",
      "Epoch 9/100\n",
      "14483/14483 [==============================] - 5s 367us/sample - loss: 0.0011 - acc: 0.9999 - val_loss: 2.7097 - val_acc: 0.6461\n",
      "Epoch 10/100\n",
      "14483/14483 [==============================] - 5s 363us/sample - loss: 5.8437e-04 - acc: 0.9999 - val_loss: 2.4189 - val_acc: 0.6289\n",
      "Epoch 11/100\n",
      "14483/14483 [==============================] - 5s 367us/sample - loss: 5.5625e-04 - acc: 0.9999 - val_loss: 2.8541 - val_acc: 0.6461\n"
     ]
    }
   ],
   "source": [
    "y_train = df_train['label'].values\n",
    "y_test = df_test['label'].values\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, mode='max', restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_text, y_train, epochs=100, batch_size=128, validation_data=(X_test_text, y_test), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.643860210008137, 0.7249405]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X_test_text)\n",
    "model.evaluate(X_test_text, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(len(preds)):\n",
    "    if(preds[i]<0.5):\n",
    "        y_pred.append(0)\n",
    "    else:\n",
    "        y_pred.append(1)\n",
    "    \n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6927639383155397"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.metrics as skl_m\n",
    "\n",
    "f1 = skl_m.f1_score(y_test, y_pred)\n",
    "f1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6ac1c65944d4ad9c277d53219687cbb170d2c6377a780c0883c4434ac8bf052"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('coursework': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
