{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\borin\\anaconda3\\envs\\coursework\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#Tweets Machine Learning Coursework \n",
    "#COMP3222\n",
    "#\n",
    "\n",
    "# To support both python 2 and 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# These two lines are required to use Tensorflow 1\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# To plot nice figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Clear tensorflow's and reset seed\n",
    "def reset_graph(seed=None):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#get data\n",
    "def getData(file_name):\n",
    "    dict_data = {}\n",
    "    data_file = open(file_name, \"r\", encoding=\"utf8\")\n",
    "    raw_data_txt = data_file.readlines()\n",
    "    raw_data = []\n",
    "    for data in raw_data_txt:\n",
    "        raw_data.append(data.split(\"\\t\"))\n",
    "    raw_data.pop(0)\n",
    "\n",
    "    target_text = [data[6] for data in raw_data]\n",
    "    dict_data['text'] = np.array([tweet[1] for tweet in raw_data])\n",
    "    dict_data['imageIds'] = np.array([tweet[3] for tweet in raw_data])\n",
    "    dict_data['timestamp'] = np.array([tweet[5] for tweet in raw_data])\n",
    "    dict_data['label'] = np.array([1 if target=='real' or target =='real\\n' else 0 for target in target_text])\n",
    "\n",
    "    return dict_data\n",
    "\n",
    "df_train = pd.DataFrame.from_dict(getData(\"mediaeval-2015-trainingset.txt\"))\n",
    "df_test = pd.DataFrame.from_dict(getData(\"mediaeval-2015-testset.txt\"))\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file pre-processing to csv\n",
    "#df_test.to_csv('test_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>imageIds</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>language</th>\n",
       "      <th>misspellings</th>\n",
       "      <th>news_company_found</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>profane</th>\n",
       "      <th>retweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>remember movie Day Tomorrow reminds happening ...</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Look Sandy NY Tremendous image hurricane Looks...</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good photo Hurricane Sandy reminds movie Indep...</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit hurricane NY</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fave place world nyc hurricane sandy statueofl...</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>en</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14478</th>\n",
       "      <td>slaps TweetDeck PigFish http co pyHcJn0jwA</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Tue Mar 11 03: 48: 36 +0000 2014</td>\n",
       "      <td>en</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14479</th>\n",
       "      <td>New Species Fish found Brazil Really good Phot...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Mon Mar 10 18: 09: 26 +0000 2014</td>\n",
       "      <td>en</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14480</th>\n",
       "      <td>call pigFISH http co 4Bml62OD15</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Mon Mar 10 10: 59: 45 +0000 2014</td>\n",
       "      <td>en</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14481</th>\n",
       "      <td>Pigfish shark pork fish http co hQzWGhyDef</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Sun Mar 09 20: 07: 10 +0000 2014</td>\n",
       "      <td>it</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14482</th>\n",
       "      <td>decide fish meat Pigfish http co 5JBtF54cmg</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Sun Mar 09 16: 36: 09 +0000 2014</td>\n",
       "      <td>en</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14483 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text        imageIds  \\\n",
       "0      remember movie Day Tomorrow reminds happening ...  sandyA_fake_46   \n",
       "1      Look Sandy NY Tremendous image hurricane Looks...  sandyA_fake_09   \n",
       "2      Good photo Hurricane Sandy reminds movie Indep...  sandyA_fake_09   \n",
       "3                                Scary shit hurricane NY  sandyA_fake_29   \n",
       "4      fave place world nyc hurricane sandy statueofl...  sandyA_fake_15   \n",
       "...                                                  ...             ...   \n",
       "14478         slaps TweetDeck PigFish http co pyHcJn0jwA      pigFish_01   \n",
       "14479  New Species Fish found Brazil Really good Phot...      pigFish_01   \n",
       "14480                    call pigFISH http co 4Bml62OD15      pigFish_01   \n",
       "14481         Pigfish shark pork fish http co hQzWGhyDef      pigFish_01   \n",
       "14482        decide fish meat Pigfish http co 5JBtF54cmg      pigFish_01   \n",
       "\n",
       "                              timestamp language  misspellings  \\\n",
       "0        Mon Oct 29 22:34:01 +0000 2012       es             0   \n",
       "1        Mon Oct 29 19:11:23 +0000 2012       es             1   \n",
       "2        Mon Oct 29 18:11:08 +0000 2012       es             1   \n",
       "3        Mon Oct 29 19:15:33 +0000 2012       en             0   \n",
       "4        Mon Oct 29 20:46:02 +0000 2012       en             2   \n",
       "...                                 ...      ...           ...   \n",
       "14478  Tue Mar 11 03: 48: 36 +0000 2014       en             4   \n",
       "14479  Mon Mar 10 18: 09: 26 +0000 2014       en             2   \n",
       "14480  Mon Mar 10 10: 59: 45 +0000 2014       en             3   \n",
       "14481  Sun Mar 09 20: 07: 10 +0000 2014       it             3   \n",
       "14482  Sun Mar 09 16: 36: 09 +0000 2014       en             3   \n",
       "\n",
       "       news_company_found  num_emoji  num_hashtags  profane  retweet  label  \n",
       "0                       0          0             1        0        0      0  \n",
       "1                       0          0             0        0        1      0  \n",
       "2                       0          0             2        0        0      0  \n",
       "3                       0          0             2        1        0      0  \n",
       "4                       0          1             4        0        0      0  \n",
       "...                   ...        ...           ...      ...      ...    ...  \n",
       "14478                   0          0             0        0        0      0  \n",
       "14479                   0          0             0        0        0      0  \n",
       "14480                   0          0             1        0        0      0  \n",
       "14481                   0          0             0        0        0      0  \n",
       "14482                   0          0             1        0        0      0  \n",
       "\n",
       "[14483 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train_v0.3.csv\")\n",
    "df_train = df_train.iloc[:, 1:]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>imageIds</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>language</th>\n",
       "      <th>profane</th>\n",
       "      <th>misspellings</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>news_company_found</th>\n",
       "      <th>retweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kereeen RT Eclipse ISS</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Fri Mar 20 09:45:43 +0000 2015</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Absolutely beautiful RT Eclipse ISS</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Fri Mar 20 11:04:02 +0000 2015</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eclipse ISS 3 20 Wow amazing</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Fri Mar 20 12:10:06 +0000 2015</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eclipse ISS</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Fri Mar 20 09:12:41 +0000 2015</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eclipse seen ISS Something else Divine creatio...</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Fri Mar 20 17:44:11 +0000 2015</td>\n",
       "      <td>fr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>ZDF presenter confesses rigged video Varoufaki...</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>Thu Mar 19 05:49:44 +0000 2015</td>\n",
       "      <td>fr</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>Oh kleine liars ZDF presenter confesses faked ...</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>Thu Mar 19 05:51:42 +0000 2015</td>\n",
       "      <td>fr</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>ZDF program confirm Varoufakis video montage</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>Thu Mar 19 09:23:21 +0000 2015</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>11 34 ALMOST noon Big confusion Varoufakis vid...</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>Thu Mar 19 10:35:20 +0000 2015</td>\n",
       "      <td>de</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>Sorry ENGLISH subtitles full English Greek sub...</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>Wed Mar 18 21:21:05 +0000 2015</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3781 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text      imageIds  \\\n",
       "0                                kereeen RT Eclipse ISS   eclipse_01    \n",
       "1                   Absolutely beautiful RT Eclipse ISS   eclipse_01    \n",
       "2                          Eclipse ISS 3 20 Wow amazing   eclipse_01    \n",
       "3                                           Eclipse ISS   eclipse_01    \n",
       "4     Eclipse seen ISS Something else Divine creatio...    eclipse_01   \n",
       "...                                                 ...           ...   \n",
       "3776  ZDF presenter confesses rigged video Varoufaki...  varoufakis_1   \n",
       "3777  Oh kleine liars ZDF presenter confesses faked ...  varoufakis_1   \n",
       "3778       ZDF program confirm Varoufakis video montage  varoufakis_1   \n",
       "3779  11 34 ALMOST noon Big confusion Varoufakis vid...  varoufakis_1   \n",
       "3780  Sorry ENGLISH subtitles full English Greek sub...  varoufakis_1   \n",
       "\n",
       "                           timestamp language  profane  misspellings  \\\n",
       "0     Fri Mar 20 09:45:43 +0000 2015       en        0             2   \n",
       "1     Fri Mar 20 11:04:02 +0000 2015       en        0             1   \n",
       "2     Fri Mar 20 12:10:06 +0000 2015       en        0             0   \n",
       "3     Fri Mar 20 09:12:41 +0000 2015       en        0             0   \n",
       "4     Fri Mar 20 17:44:11 +0000 2015       fr        0             0   \n",
       "...                              ...      ...      ...           ...   \n",
       "3776  Thu Mar 19 05:49:44 +0000 2015       fr        0             2   \n",
       "3777  Thu Mar 19 05:51:42 +0000 2015       fr        0             2   \n",
       "3778  Thu Mar 19 09:23:21 +0000 2015       es        0             2   \n",
       "3779  Thu Mar 19 10:35:20 +0000 2015       de        0             4   \n",
       "3780  Wed Mar 18 21:21:05 +0000 2015       en        0             1   \n",
       "\n",
       "      num_emoji  num_hashtags  news_company_found  retweet  label  \n",
       "0             0             0                   0        1      0  \n",
       "1             0             0                   0        1      0  \n",
       "2             0             0                   0        0      0  \n",
       "3             0             0                   0        0      0  \n",
       "4             1             0                   0        0      0  \n",
       "...         ...           ...                 ...      ...    ...  \n",
       "3776          0             1                   0        0      0  \n",
       "3777          0             0                   0        0      0  \n",
       "3778          0             0                   0        0      0  \n",
       "3779          2             0                   0        0      0  \n",
       "3780          0             1                   0        0      0  \n",
       "\n",
       "[3781 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"test_v0.3.csv\")\n",
    "df_test = df_test.iloc[:, 1:]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find all the different languages\n",
    "#https://meta.wikimedia.org/wiki/Template:List_of_language_names_ordered_by_code conversion of symbol to langauge table\n",
    "from langdetect import detect\n",
    "\n",
    "def language_detect(data_frame):\n",
    "    languages = []\n",
    "    for tweet in data_frame['text'].values:\n",
    "        try:\n",
    "            languages.append(detect(tweet))\n",
    "        except:\n",
    "            languages.append('en')\n",
    "\n",
    "    data_frame.insert(3, \"language\", languages, True)\n",
    "\n",
    "    languages_seen = set(languages)\n",
    "    print(\"Languages seen: \", languages_seen)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retweet_detection(data_frame):\n",
    "    retweets = [1 if tweet.count(\"RT\") >= 1 else 0 for tweet in data_frame[\"text\"].values]\n",
    "    data_frame.insert(4, \"retweet\", retweets, True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def translate(data_frame):\n",
    "    translate_text = {}\n",
    "    for i in range(len(data_frame)):\n",
    "        if(data_frame.iloc[i]['language'] != 'en'):\n",
    "            translate_text[data_frame.iloc[i]['text']] = GoogleTranslator(source='auto', target='en').translate(data_frame.iloc[i]['text'])\n",
    "            #print(i, data_frame.iloc[i]['language'], GoogleTranslator(source='auto', target='en').translate(df_train.iloc[i]['text']))\n",
    "\n",
    "    data_frame[\"text\"].replace(translate_text, inplace=True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>imageIds</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿Se acuerdan de la película: “El día después d...</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda i...</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buena la foto del Huracán Sandy, me recuerda a...</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My fave place in the world #nyc #hurricane #sa...</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14478</th>\n",
       "      <td>@BobombDom *slaps TweetDeck with the PigFish h...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Tue Mar 11 03: 48: 36 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14479</th>\n",
       "      <td>New Species of Fish found in Brazil or just Re...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Mon Mar 10 18: 09: 26 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14480</th>\n",
       "      <td>What do we call this? #pigFISH http: \\/\\/t.co\\...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Mon Mar 10 10: 59: 45 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14481</th>\n",
       "      <td>Pigfish ? E dopo il pescecane c'è il pesce mai...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Sun Mar 09 20: 07: 10 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14482</th>\n",
       "      <td>For those who can't decide between fish or mea...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Sun Mar 09 16: 36: 09 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14483 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text        imageIds  \\\n",
       "0      ¿Se acuerdan de la película: “El día después d...  sandyA_fake_46   \n",
       "1      @milenagimon: Miren a Sandy en NY!  Tremenda i...  sandyA_fake_09   \n",
       "2      Buena la foto del Huracán Sandy, me recuerda a...  sandyA_fake_09   \n",
       "3         Scary shit #hurricane #NY http://t.co/e4JLBUfH  sandyA_fake_29   \n",
       "4      My fave place in the world #nyc #hurricane #sa...  sandyA_fake_15   \n",
       "...                                                  ...             ...   \n",
       "14478  @BobombDom *slaps TweetDeck with the PigFish h...      pigFish_01   \n",
       "14479  New Species of Fish found in Brazil or just Re...      pigFish_01   \n",
       "14480  What do we call this? #pigFISH http: \\/\\/t.co\\...      pigFish_01   \n",
       "14481  Pigfish ? E dopo il pescecane c'è il pesce mai...      pigFish_01   \n",
       "14482  For those who can't decide between fish or mea...      pigFish_01   \n",
       "\n",
       "                              timestamp  label  \n",
       "0        Mon Oct 29 22:34:01 +0000 2012      0  \n",
       "1        Mon Oct 29 19:11:23 +0000 2012      0  \n",
       "2        Mon Oct 29 18:11:08 +0000 2012      0  \n",
       "3        Mon Oct 29 19:15:33 +0000 2012      0  \n",
       "4        Mon Oct 29 20:46:02 +0000 2012      0  \n",
       "...                                 ...    ...  \n",
       "14478  Tue Mar 11 03: 48: 36 +0000 2014      0  \n",
       "14479  Mon Mar 10 18: 09: 26 +0000 2014      0  \n",
       "14480  Mon Mar 10 10: 59: 45 +0000 2014      0  \n",
       "14481  Sun Mar 09 20: 07: 10 +0000 2014      0  \n",
       "14482  Sun Mar 09 16: 36: 09 +0000 2014      0  \n",
       "\n",
       "[14483 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from profanity_filter import ProfanityFilter\n",
    "\n",
    "def profanity_detection(data_frame):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    profanity_filter = ProfanityFilter(nlps={'en': nlp})  # reuse spacy Language (optional)\n",
    "    nlp.add_pipe(profanity_filter.spacy_component, last=True)\n",
    "\n",
    "    doc = nlp('This is fuck shit!')\n",
    "\n",
    "    doc._.is_profane\n",
    "    profanity_check = []\n",
    "    count = 0\n",
    "    for tweet in data_frame['text'].values:\n",
    "        text = nlp(tweet)\n",
    "        if(text._.is_profane):\n",
    "            profanity_check.append(1)\n",
    "        else: \n",
    "            profanity_check.append(0)\n",
    "        \n",
    "        #print(count, text._.is_profane)\n",
    "        count += 1\n",
    "    \n",
    "    data_frame.insert(4, \"profane\", profanity_check, True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numofhashtags(data_frame):\n",
    "    num_hashtags = []\n",
    "    for i in range(len(data_frame)):\n",
    "        num_hashtags.append(data_frame.iloc[i]['text'].count(\"#\"))\n",
    "    #maybe count number of punctuation as well.\n",
    "    data_frame.insert(4, \"num_hashtags\", num_hashtags, True)\n",
    "    return data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def emoji_detect(data_frame):\n",
    "    num_emojis = []\n",
    "    for i in range(len(data_frame)):\n",
    "        emoji_count = emoji.emoji_count(data_frame.iloc[i]['text'])\n",
    "        num_emojis.append(emoji_count)\n",
    "    \n",
    "    data_frame.insert(4, \"num_emoji\", num_emojis, True)\n",
    "    return data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def parseText(data_frame):\n",
    "    parsed_text = {}\n",
    "    for i in range(len(data_frame)):\n",
    "        parsed = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",data_frame.iloc[i]['text']).split())\n",
    "        parsed_text[data_frame.iloc[i]['text']] = parsed\n",
    "        #print(i, parsed)\n",
    "    data_frame[\"text\"].replace(parsed_text, inplace=True)\n",
    "    return data_frame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based of https://craft.co/cnn/competitors\n",
    "#\n",
    "def new_companies(data_frame):\n",
    "    news_companies =[\"cnn\", \"bbc\", \"nbc\", \"new york times\", \"wall street journal\", \" ap \", \"fox\", \"cnbc\", \"daily mail\"]\n",
    "    news_found = []\n",
    "    for tweet in data_frame['text'].values:\n",
    "        tweet = tweet.lower()\n",
    "        if(any(news in tweet for news in news_companies) or (tweet.find('ap') == 0) or (tweet.find('ap') == len(tweet)-2) ) :\n",
    "            news_found.append(1)\n",
    "        else:\n",
    "            news_found.append(0)\n",
    "    data_frame.insert(4, \"news_company_found\", news_found, True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States\n",
      "Fave, Jalan Cihampelas, Cipaganti, Coblong, Bandung, Jawa Barat, 40131, Indonesia\n",
      "Placé, Mayenne, Pays de la Loire, France métropolitaine, 53240, France\n",
      "India\n",
      "The (Oval) Banqueting Suite, 372, Stratford Road, Moseley, Sparkhill, Birmingham, West Midlands Combined Authority, West Midlands, England, B11 4AB, United Kingdom\n",
      "World, 어울마당로, 홍대거리, 서교동, 마포구, 서울, 04039, 대한민국\n",
      "New York, United States\n",
      "Hurricane, City Of Karratha, Western Australia, Australia\n",
      "Sandy, Salt Lake County, Utah, United States\n",
      "Кожай-Семёновка, Кожай-Семеновский сельсовет, Миякинский район, Башкортостан, Приволжский федеральный округ, Россия\n",
      "Barkman Sqaure, Midland, Midland County, Texas, 79705, United States\n"
     ]
    }
   ],
   "source": [
    "#location detector too much noise picked up\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"COMP322-MLT\")\n",
    "x = \"My fave place in the world nyc hurricane sandy times sqaure\"\n",
    "x = x.split(' ')\n",
    "for i in range(len(x)):\n",
    "    try:\n",
    "        location = geolocator.geocode(x[i])\n",
    "        print(location.address)\n",
    "    except:\n",
    "        print(\"Notfound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run hashtags if first time running on machine\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def stopWordRemoval(data_frame):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = {}\n",
    "    for i in range(len(data_frame)):\n",
    "        word_tokens = word_tokenize(data_frame.iloc[i]['text'])\n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "        new_sentence = \" \".join(filtered_sentence)\n",
    "        filtered_text[data_frame.iloc[i]['text']] = new_sentence\n",
    "        #print(i, new_sentence)\n",
    "    data_frame[\"text\"].replace(filtered_text, inplace=True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "def misspelling_detection(data_frame):\n",
    "    spell = SpellChecker()\n",
    "    misspelled_count = []\n",
    "    for i in range(len(data_frame)):\n",
    "        word_list = word_tokenize(data_frame.iloc[i]['text'])\n",
    "        misspelled = spell.unknown(word_list)\n",
    "        misspelled_count.append(len(misspelled))\n",
    "        #print(i, len(misspelled), misspelled))\n",
    "    data_frame.insert(4, \"misspellings\", misspelled_count, True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages seen:  {'ta', 'sq', 'lt', 'el', 'ar', 'so', 'tl', 'pl', 'hi', 'de', 'ro', 'ru', 'th', 'fr', 'ko', 'es', 'ca', 'en', 'ja', 'pt', 'af', 'id', 'cy', 'nl', 'et', 'it', 'fi', 'sv', 'te', 'vi', 'tr', 'bg', 'hr'}\n",
      "Language Detection Done\n",
      "Translation Done\n",
      "Retweet Detection Done\n",
      "News companies Detection Done\n",
      "Hashtag count Done\n",
      "Emoji count Done\n",
      "Parsing Done\n",
      "Stopwprd removal Done\n",
      "Misspelling Count Done\n",
      "Profanity Detection Done\n",
      "Preprocessing done\n"
     ]
    }
   ],
   "source": [
    "df_test = language_detect(df_test)\n",
    "print(\"Language Detection Done\")\n",
    "df_test = translate(df_test)\n",
    "print(\"Translation Done\")\n",
    "df_test = retweet_detection(df_test)\n",
    "print(\"Retweet Detection Done\")\n",
    "df_test = new_companies(df_test)\n",
    "print(\"News companies Detection Done\")\n",
    "df_test = numofhashtags(df_test)\n",
    "print(\"Hashtag count Done\")\n",
    "df_test = emoji_detect(df_test)\n",
    "print(\"Emoji count Done\")\n",
    "df_test = parseText(df_test)\n",
    "print(\"Parsing Done\")\n",
    "df_test = stopWordRemoval(df_test)\n",
    "print(\"Stopwprd removal Done\")\n",
    "df_test = misspelling_detection(df_test)\n",
    "print(\"Misspelling Count Done\")\n",
    "df_test = profanity_detection(df_test)\n",
    "print(\"Profanity Detection Done\")\n",
    "\n",
    "print(\"Preprocessing done\")\n",
    "#before passing the text as unigram .lower everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>imageIds</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>language</th>\n",
       "      <th>profane</th>\n",
       "      <th>misspellings</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>news_company_found</th>\n",
       "      <th>retweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kereeen RT Eclipse ISS</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Fri Mar 20 09:45:43 +0000 2015</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Absolutely beautiful RT Eclipse ISS</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Fri Mar 20 11:04:02 +0000 2015</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eclipse ISS 3 20 Wow amazing</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Fri Mar 20 12:10:06 +0000 2015</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eclipse ISS</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Fri Mar 20 09:12:41 +0000 2015</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eclipse seen ISS Something else Divine creatio...</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Fri Mar 20 17:44:11 +0000 2015</td>\n",
       "      <td>fr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>ZDF presenter confesses rigged video Varoufaki...</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>Thu Mar 19 05:49:44 +0000 2015</td>\n",
       "      <td>fr</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>Oh kleine liars ZDF presenter confesses faked ...</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>Thu Mar 19 05:51:42 +0000 2015</td>\n",
       "      <td>fr</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>ZDF program confirm Varoufakis video montage</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>Thu Mar 19 09:23:21 +0000 2015</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>11 34 ALMOST noon Big confusion Varoufakis vid...</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>Thu Mar 19 10:35:20 +0000 2015</td>\n",
       "      <td>de</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>Sorry ENGLISH subtitles full English Greek sub...</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>Wed Mar 18 21:21:05 +0000 2015</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3781 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text      imageIds  \\\n",
       "0                                kereeen RT Eclipse ISS   eclipse_01    \n",
       "1                   Absolutely beautiful RT Eclipse ISS   eclipse_01    \n",
       "2                          Eclipse ISS 3 20 Wow amazing   eclipse_01    \n",
       "3                                           Eclipse ISS   eclipse_01    \n",
       "4     Eclipse seen ISS Something else Divine creatio...    eclipse_01   \n",
       "...                                                 ...           ...   \n",
       "3776  ZDF presenter confesses rigged video Varoufaki...  varoufakis_1   \n",
       "3777  Oh kleine liars ZDF presenter confesses faked ...  varoufakis_1   \n",
       "3778       ZDF program confirm Varoufakis video montage  varoufakis_1   \n",
       "3779  11 34 ALMOST noon Big confusion Varoufakis vid...  varoufakis_1   \n",
       "3780  Sorry ENGLISH subtitles full English Greek sub...  varoufakis_1   \n",
       "\n",
       "                           timestamp language  profane  misspellings  \\\n",
       "0     Fri Mar 20 09:45:43 +0000 2015       en        0             2   \n",
       "1     Fri Mar 20 11:04:02 +0000 2015       en        0             1   \n",
       "2     Fri Mar 20 12:10:06 +0000 2015       en        0             0   \n",
       "3     Fri Mar 20 09:12:41 +0000 2015       en        0             0   \n",
       "4     Fri Mar 20 17:44:11 +0000 2015       fr        0             0   \n",
       "...                              ...      ...      ...           ...   \n",
       "3776  Thu Mar 19 05:49:44 +0000 2015       fr        0             2   \n",
       "3777  Thu Mar 19 05:51:42 +0000 2015       fr        0             2   \n",
       "3778  Thu Mar 19 09:23:21 +0000 2015       es        0             2   \n",
       "3779  Thu Mar 19 10:35:20 +0000 2015       de        0             4   \n",
       "3780  Wed Mar 18 21:21:05 +0000 2015       en        0             1   \n",
       "\n",
       "      num_emoji  num_hashtags  news_company_found  retweet  label  \n",
       "0             0             0                   0        1      0  \n",
       "1             0             0                   0        1      0  \n",
       "2             0             0                   0        0      0  \n",
       "3             0             0                   0        0      0  \n",
       "4             1             0                   0        0      0  \n",
       "...         ...           ...                 ...      ...    ...  \n",
       "3776          0             1                   0        0      0  \n",
       "3777          0             0                   0        0      0  \n",
       "3778          0             0                   0        0      0  \n",
       "3779          2             0                   0        0      0  \n",
       "3780          0             1                   0        0      0  \n",
       "\n",
       "[3781 rows x 11 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>imageIds</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>language</th>\n",
       "      <th>profane</th>\n",
       "      <th>misspellings</th>\n",
       "      <th>num_emoji</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>news_company_found</th>\n",
       "      <th>retweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>remember movie Day Tomorrow reminds happening ...</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Look Sandy NY Tremendous image hurricane Looks...</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good photo Hurricane Sandy reminds movie Indep...</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scary shit hurricane NY</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fave place world nyc hurricane sandy statueofl...</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14478</th>\n",
       "      <td>slaps TweetDeck PigFish http co pyHcJn0jwA</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Tue Mar 11 03: 48: 36 +0000 2014</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14479</th>\n",
       "      <td>New Species Fish found Brazil Really good Phot...</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Mon Mar 10 18: 09: 26 +0000 2014</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14480</th>\n",
       "      <td>call pigFISH http co 4Bml62OD15</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Mon Mar 10 10: 59: 45 +0000 2014</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14481</th>\n",
       "      <td>Pigfish shark pork fish http co hQzWGhyDef</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Sun Mar 09 20: 07: 10 +0000 2014</td>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14482</th>\n",
       "      <td>decide fish meat Pigfish http co 5JBtF54cmg</td>\n",
       "      <td>pigFish_01</td>\n",
       "      <td>Sun Mar 09 16: 36: 09 +0000 2014</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14483 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text        imageIds  \\\n",
       "0      remember movie Day Tomorrow reminds happening ...  sandyA_fake_46   \n",
       "1      Look Sandy NY Tremendous image hurricane Looks...  sandyA_fake_09   \n",
       "2      Good photo Hurricane Sandy reminds movie Indep...  sandyA_fake_09   \n",
       "3                                Scary shit hurricane NY  sandyA_fake_29   \n",
       "4      fave place world nyc hurricane sandy statueofl...  sandyA_fake_15   \n",
       "...                                                  ...             ...   \n",
       "14478         slaps TweetDeck PigFish http co pyHcJn0jwA      pigFish_01   \n",
       "14479  New Species Fish found Brazil Really good Phot...      pigFish_01   \n",
       "14480                    call pigFISH http co 4Bml62OD15      pigFish_01   \n",
       "14481         Pigfish shark pork fish http co hQzWGhyDef      pigFish_01   \n",
       "14482        decide fish meat Pigfish http co 5JBtF54cmg      pigFish_01   \n",
       "\n",
       "                              timestamp language  profane  misspellings  \\\n",
       "0        Mon Oct 29 22:34:01 +0000 2012       es        0             0   \n",
       "1        Mon Oct 29 19:11:23 +0000 2012       es        0             1   \n",
       "2        Mon Oct 29 18:11:08 +0000 2012       es        0             1   \n",
       "3        Mon Oct 29 19:15:33 +0000 2012       en        1             0   \n",
       "4        Mon Oct 29 20:46:02 +0000 2012       en        0             2   \n",
       "...                                 ...      ...      ...           ...   \n",
       "14478  Tue Mar 11 03: 48: 36 +0000 2014       en        0             4   \n",
       "14479  Mon Mar 10 18: 09: 26 +0000 2014       en        0             2   \n",
       "14480  Mon Mar 10 10: 59: 45 +0000 2014       en        0             3   \n",
       "14481  Sun Mar 09 20: 07: 10 +0000 2014       it        0             3   \n",
       "14482  Sun Mar 09 16: 36: 09 +0000 2014       en        0             3   \n",
       "\n",
       "       num_emoji  num_hashtags  news_company_found  retweet  label  \n",
       "0              0             1                   0        0      0  \n",
       "1              0             0                   0        1      0  \n",
       "2              0             2                   0        0      0  \n",
       "3              0             2                   0        0      0  \n",
       "4              1             4                   0        0      0  \n",
       "...          ...           ...                 ...      ...    ...  \n",
       "14478          0             0                   0        0      0  \n",
       "14479          0             0                   0        0      0  \n",
       "14480          0             1                   0        0      0  \n",
       "14481          0             0                   0        0      0  \n",
       "14482          0             1                   0        0      0  \n",
       "\n",
       "[14483 rows x 11 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train[['text', 'imageIds', 'timestamp', 'language', 'profane', 'misspellings', 'num_emoji', 'num_hashtags', 'news_company_found', 'retweet', 'label']]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_features(data_frame):\n",
    "    features = []\n",
    "    for i in range(len(data_frame)):\n",
    "        temp = []\n",
    "        temp.append(data_frame.iloc[i]['profane'])\n",
    "        temp.append(data_frame.iloc[i]['misspellings'])\n",
    "        temp.append(data_frame.iloc[i]['num_emoji'])\n",
    "        temp.append(data_frame.iloc[i]['num_hashtags'])\n",
    "        temp.append(data_frame.iloc[i]['news_company_found'])\n",
    "        features.append(temp)\n",
    "    features = np.array(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = pull_features(df_train)\n",
    "X_test_features = pull_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\borin\\anaconda3\\envs\\coursework\\lib\\site-packages\\tensorflow\\python\\keras\\initializers\\initializers_v1.py:58: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "features (InputLayer)        [(None, 5)]               0         \n",
      "_________________________________________________________________\n",
      "article_body_embedding (Embe (None, 5, 50)             4000      \n",
      "_________________________________________________________________\n",
      "article_body_conv (Conv1D)   (None, 1, 20)             5020      \n",
      "_________________________________________________________________\n",
      "article_body_pooling (Global (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 9,291\n",
      "Trainable params: 9,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text_input = tf.keras.layers.Input(shape=(5,), name='features')\n",
    "text_embed = tf.keras.layers.Embedding(79 + 1, 50, input_length=5, name='article_body_embedding')(text_input)\n",
    "text_conv = tf.keras.layers.Conv1D(20, 5, name='article_body_conv')(text_embed)\n",
    "text_pool = tf.keras.layers.GlobalMaxPool1D(name='article_body_pooling')(text_conv)\n",
    "#concat = tf.keras.layers.concatenate([text_pool])\n",
    "dense_100 = tf.keras.layers.Dense(10, activation='relu')(text_pool)\n",
    "dense_50 = tf.keras.layers.Dense(5, activation='relu')(dense_100)\n",
    "out_layer = tf.keras.layers.Dense(1, activation='sigmoid')(dense_50)\n",
    "model = tf.keras.models.Model(inputs=[text_input], outputs=out_layer)\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14483 samples, validate on 3781 samples\n",
      "Epoch 1/100\n",
      "13312/14483 [==========================>...] - ETA: 0s - loss: 0.6529 - acc: 0.6454WARNING:tensorflow:From C:\\Users\\borin\\anaconda3\\envs\\coursework\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "14483/14483 [==============================] - 0s 29us/sample - loss: 0.6508 - acc: 0.6459 - val_loss: 0.6218 - val_acc: 0.6781\n",
      "Epoch 2/100\n",
      "14483/14483 [==============================] - 0s 22us/sample - loss: 0.6185 - acc: 0.6546 - val_loss: 0.6743 - val_acc: 0.6779\n",
      "Epoch 3/100\n",
      "14483/14483 [==============================] - 0s 21us/sample - loss: 0.6146 - acc: 0.6557 - val_loss: 0.6786 - val_acc: 0.6776\n",
      "Epoch 4/100\n",
      "14483/14483 [==============================] - 0s 21us/sample - loss: 0.6136 - acc: 0.6558 - val_loss: 0.6840 - val_acc: 0.6776\n",
      "Epoch 5/100\n",
      "14483/14483 [==============================] - 0s 17us/sample - loss: 0.6131 - acc: 0.6559 - val_loss: 0.6562 - val_acc: 0.6776\n",
      "Epoch 6/100\n",
      "14483/14483 [==============================] - 0s 21us/sample - loss: 0.6129 - acc: 0.6559 - val_loss: 0.6718 - val_acc: 0.6776\n"
     ]
    }
   ],
   "source": [
    "y_train = df_train['label'].values\n",
    "y_test = df_test['label'].values\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, mode='max', restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_features, y_train, epochs=100, batch_size=128, validation_data=(X_test_features, y_test), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6218266290226048, 0.67812747]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X_test_features)\n",
    "model.evaluate(X_test_features, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3781, 1)\n",
      "(3781,)\n"
     ]
    }
   ],
   "source": [
    "preds.shape\n",
    "#y_test = np.array(y_test)\n",
    "print(preds.shape)\n",
    "tf.reshape(preds, [-1, 1])\n",
    "tf.reshape(y_test, [-1, 1])\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds\n",
    "y_pred = []\n",
    "for i in range(len(preds)):\n",
    "    if(preds[i]<0.5):\n",
    "        y_pred.append(0)\n",
    "    else:\n",
    "        y_pred.append(1)\n",
    "    \n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6781274795027771"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.metrics as skl_m\n",
    "\n",
    "f1 = skl_m.f1_score(y_test, y_pred, average='micro')\n",
    "f1\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6ac1c65944d4ad9c277d53219687cbb170d2c6377a780c0883c4434ac8bf052"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('coursework': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
